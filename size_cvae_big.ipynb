{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time         \n",
    "from sklearn import metrics\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchsummary import summary\n",
    "from utils.get_dataset import *\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "import datetime\n",
    "import random  \n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, hidden_dims, latent_dim):\n",
    "        super(SizeEncoder, self).__init__()\n",
    "        self.encoder = nn.ModuleList()\n",
    "        in_dim = input_dim + condition_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.encoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim),\n",
    "                    nn.ReLU())\n",
    "                    # ,nn.LayerNorm(h_dim))\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor, c: Tensor) -> List[Tensor]:\n",
    "        x = torch.cat((x, c), dim=1)\n",
    "        for module in self.encoder:\n",
    "            x = module(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "        return [mu, log_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SizeDecoder(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, condition_dim, hidden_dims, output_dim):\n",
    "        super(SizeDecoder, self).__init__()\n",
    "        self.decoder = torch.nn.ModuleList()\n",
    "        in_dim = latent_dim + condition_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.decoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim,),\n",
    "                    nn.ReLU())\n",
    "                    # nn.LayerNorm(h_dim))\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.output = nn.Linear(hidden_dims[-1], output_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor, c: Tensor) -> List[Tensor]:\n",
    "        x = torch.cat((x, c), dim=1)\n",
    "        for module in self.decoder:\n",
    "            x = module(x)\n",
    "        result = self.output(x)\n",
    "        result = F.softmax(result, dim=1)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps * std + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, dataset, optimizer):\n",
    "    dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    epoch_loss, epoch_kld, epoch_recon, sample_num, epoch_max_loss = 0, 0, 0, 0, 0\n",
    "    max_loss_weight = 0.1\n",
    "    for size_data, condition in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        size_data, condition = size_data.float().to(device), condition.float().to(device)\n",
    "        mu, var = encoder(size_data, condition)\n",
    "        z = reparameterize(mu, var)#z.shape=(batch_size,latent_dim)\n",
    "        y = decoder(z, condition)\n",
    "        recon_loss = F.l1_loss(y, size_data)\n",
    "        max_recon_loss = torch.max(torch.abs(y - size_data).mean(dim=1))\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + var - mu ** 2 - var.exp(), dim = 1), dim = 0)\n",
    "        loss = recon_loss + max_loss_weight * max_recon_loss + kld_weight * kld_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * len(size_data)\n",
    "        epoch_kld += kld_loss.item() * len(size_data)\n",
    "        epoch_recon += recon_loss.item() * len(size_data)\n",
    "        epoch_max_loss = max(epoch_max_loss, max_recon_loss.item())\n",
    "        sample_num += len(size_data)\n",
    "\n",
    "    epoch_loss /= sample_num\n",
    "    epoch_recon /= sample_num\n",
    "    epoch_kld /= sample_num\n",
    "    return epoch_loss, epoch_recon, epoch_kld, epoch_max_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(decoder,sizedata,locality,latent_dim,locality_onehots_dict,step=0, test_size=100):\n",
    "    t0=time.time()\n",
    "    # bins=20\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        condition = random.sample(locality_onehots_dict.keys(),min(len(locality_onehots_dict.keys()),500))\n",
    "        condition = [eval(i) for i in condition]\n",
    "        condition=torch.Tensor(np.array(condition)).float().to(device)\n",
    "        c_min_95 = []\n",
    "        c_min_aver = []\n",
    "        r_min_95 = []\n",
    "        r_min_aver = []\n",
    "        coverage = []\n",
    "        for cond in condition:\n",
    "            z=torch.randn([test_size,latent_dim]).to(device)\n",
    "            y=decoder(z,cond.expand(test_size,-1)).cpu()\n",
    "        # print(y.shape)#y.shape=[test_size, 类别数=65]\n",
    "            locality_key = str(list(np.array(cond.cpu())))\n",
    "            matrix=torch.zeros((test_size,len(locality_onehots_dict[locality_key])))\n",
    "            for i in range(test_size):\n",
    "                for j in range(len(locality_onehots_dict[locality_key])): \n",
    "                    matrix[i,j]=torch.max(np.abs(y[i]-sizedata[locality_onehots_dict[locality_key][j]]))   \n",
    "            # for j in range(len(dataset)):\n",
    "            #     matrix[i][j] = sum(abs(y[i]-dataset[j][0]))/dataset[j][0].shape[0]\n",
    "            column_min,column_posi=torch.min(matrix,dim=0)\n",
    "            row_min,row_posi=torch.min(matrix,dim=1)\n",
    "            c_min_95.append(np.percentile(column_min,95))\n",
    "            c_min_aver.append(np.mean(np.array(column_min)))\n",
    "            r_min_95.append(np.percentile(row_min,95))\n",
    "            r_min_aver.append(np.mean(np.array(row_min)))\n",
    "            row_posi=torch.unique(row_posi)\n",
    "            coverage.append(len(row_posi)/len(locality_onehots_dict[locality_key]))\n",
    "        # plt.hist(column_min,bins=bins,density=True,cumulative=True,color='blue')\n",
    "        # plt.hist(row_min,bins=bins,density=True,cumulative=True,color='yellow')\n",
    "        # plt.savefig('result/{date}/'.format(date=date)+str(step)+\".png\")\n",
    "\n",
    "        # print(\"coverage for testsize \"+str(test_size)+\" is :\"+str(len(row_posi)/test_size))\n",
    "        print(\"eval in\"+str(time.time()-t0)+\" //coverage is %.2f on average and is %.2f for the worst\" %\n",
    "              (np.mean(coverage), np.percentile(coverage,1)) +\n",
    "              \" //per sample error is %.2f on average and is %.2f for the worst\" %(np.mean(r_min_aver),np.percentile(r_min_95,95)) +\n",
    "              \" //per source data error is %.2f on average and is %.2f for the worst\" %(np.mean(c_min_aver),np.percentile(c_min_95,95)))\n",
    "        # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_locality(pairdata, freqpairs,pairsize):\n",
    "    latent_size = 128\n",
    "    \n",
    "    src_count, dst_count = defaultdict(int), defaultdict(int)\n",
    "    src_ip, dst_ip = [], []\n",
    "\n",
    "    for pair in freqpairs:\n",
    "        src_ip.append(pairdata[pair]['srcip'][0])\n",
    "        dst_ip.append(pairdata[pair]['dstip'][0])\n",
    "        src_count[pairdata[pair]['srcip'][0]] += 1\n",
    "        dst_count[pairdata[pair]['dstip'][0]] += 1\n",
    "\n",
    "    max_src_count = np.max(list(src_count.values()))\n",
    "    max_dst_count = np.max(list(dst_count.values()))\n",
    "    print(\"maxsrccount\"+str(max_src_count)+\"maxdstcount\"+str(max_dst_count))\n",
    "    condition_size = max_src_count + max_dst_count + 2\n",
    "\n",
    "    locality_strings = []\n",
    "    locality_onehots = []\n",
    "    locality_onehots_dict=defaultdict(list)\n",
    "    i = 0\n",
    "    for pair in freqpairs:\n",
    "        locality_strings.append(\"{src},{dst}\".format(src=src_count[pairdata[pair]['srcip'][0]], dst=dst_count[pairdata[pair]['dstip'][0]]))\n",
    "        locality_onehot = np.zeros(condition_size)\n",
    "        locality_onehot[src_count[pairdata[pair]['srcip'][0]]] = 1\n",
    "        locality_onehot[max_src_count + dst_count[pairdata[pair]['dstip'][0]]] = 1\n",
    "        locality_onehots.append(locality_onehot)\n",
    "        # locality_onehot.dtype=int\n",
    "        locality_onehots_dict[str(list(locality_onehot))].append(i)\n",
    "        i+=1\n",
    "    values, counts = np.unique(locality_strings, return_counts=True)\n",
    "    pair_counts = dict(zip(values, counts))\n",
    "\n",
    "    return locality_strings, np.array(locality_onehots), pair_counts, condition_size, locality_onehots_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data in 7m 10s\n",
      "maxsrccount106maxdstcount129\n",
      "get locality in 7m 34s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "random.seed(114514)\n",
    "# read data\n",
    "traces = 100000\n",
    "pairdata, freqpairs, n_size, n_interval,pairsize = get_fb_data(traces)\n",
    "sizedata = get_data(pairdata, freqpairs, 'size_index', n_size)\n",
    "intervaldata = get_data(pairdata, freqpairs, 'interval_index', n_interval)\n",
    "print('read data in %dm %ds' % ((time.time() - t0) / 60, (time.time() - t0) % 60))\n",
    "\n",
    "# get locality\n",
    "locality_strings, locality_onehots, pair_counts, condition_size, locality_onehots_dict = get_locality(pairdata, freqpairs,pairsize)\n",
    "print('get locality in %dm %ds' % ((time.time() - t0) / 60, (time.time() - t0) % 60))\n",
    "dataset = [pair for pair in zip(sizedata, locality_onehots)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dims = [768, 512, 256, 128, 64]\n",
    "latent_dim = 32\n",
    "encoder = SizeEncoder(n_size, condition_size, hidden_dims, latent_dim).to(device)\n",
    "hidden_dims.reverse()\n",
    "decoder = SizeDecoder(latent_dim, condition_size, hidden_dims, n_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start in: 2023/11/21 07:33:21\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "kld_weight = 1e-4\n",
    "optimizer = torch.optim.Adam([{'params': encoder.parameters()}, {'params': decoder.parameters()}], lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.9)\n",
    "print('start in:', time.strftime(\"%Y/%m/%d %H:%M:%S\", time.localtime()))\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.now()\n",
    "date = '%s-%s-%s-%s' % (date.year, date.month, date.day, date.hour)\n",
    "if os.path.exists('model/{date}/'.format(date=date)):\n",
    "    os.system('rm -r model/{date}/'.format(date=date))\n",
    "os.makedirs('model/{date}/'.format(date=date))\n",
    "if os.path.exists('result/{date}/'.format(date=date)):\n",
    "    os.system('rm -r result/{date}/'.format(date=date))\n",
    "os.makedirs('result/{date}/'.format(date=date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=100, avg_loss=4.22e-03, kld=2.04, recon=2.77e-03(max=2.02e-02), time=457.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_992884/2939991403.py:6: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  condition = random.sample(locality_onehots_dict.keys(),min(len(locality_onehots_dict.keys()),500))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval in90.08913683891296 //coverage is 0.74 on average and is 0.20 for the worst //per sample error is 0.05 on average and is 0.18 for the worst //per source data error is 0.03 on average and is 0.08 for the worst\n",
      "save model\n",
      "epoch=200, avg_loss=3.68e-03, kld=2.19, recon=2.69e-03(max=1.57e-02), time=928.23\n",
      "eval in47.36586308479309 //coverage is 0.77 on average and is 0.28 for the worst //per sample error is 0.04 on average and is 0.19 for the worst //per source data error is 0.03 on average and is 0.08 for the worst\n",
      "save model\n",
      "epoch=300, avg_loss=3.43e-03, kld=3.10, recon=2.48e-03(max=1.34e-02), time=1361.88\n",
      "eval in79.21932983398438 //coverage is 0.85 on average and is 0.30 for the worst //per sample error is 0.04 on average and is 0.20 for the worst //per source data error is 0.03 on average and is 0.07 for the worst\n",
      "save model\n",
      "epoch=400, avg_loss=3.28e-03, kld=3.30, recon=2.41e-03(max=1.17e-02), time=1923.29\n",
      "eval in92.01436400413513 //coverage is 0.84 on average and is 0.37 for the worst //per sample error is 0.04 on average and is 0.17 for the worst //per source data error is 0.03 on average and is 0.09 for the worst\n",
      "save model\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "avg_loss = 0\n",
    "min_loss = 1e9\n",
    "print_every = 100\n",
    "for epoch in range(100001):\n",
    "    epoch_loss, epoch_recon, epoch_kld, max_loss = train(encoder, decoder, dataset, optimizer)\n",
    "    avg_loss += epoch_loss\n",
    "    if epoch and epoch % print_every == 0:\n",
    "        avg_loss /= print_every\n",
    "        cur_time = time.time()\n",
    "        print(\"epoch=%d, avg_loss=%.2e, kld=%.2f, recon=%.2e(max=%.2e), time=%.2f\" % (epoch, avg_loss, epoch_kld, epoch_recon, max_loss, cur_time - start_time))\n",
    "        if epoch % (print_every*10) == 0:\n",
    "            evaluate(decoder,sizedata,locality_onehots,latent_dim,locality_onehots_dict,epoch,1000)\n",
    "        else:\n",
    "            evaluate(decoder,sizedata,locality_onehots,latent_dim,locality_onehots_dict,epoch,100)\n",
    "        if avg_loss < min_loss:\n",
    "            min_loss = avg_loss\n",
    "            torch.save(encoder, 'model/{date}/encoder.pth'.format(date=date))\n",
    "            torch.save(decoder, 'model/{date}/decoder.pth'.format(date=date))\n",
    "            print('save model')\n",
    "        sys.stdout.flush()\n",
    "        if avg_loss < 1e-3:\n",
    "            break\n",
    "        avg_loss = 0\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
