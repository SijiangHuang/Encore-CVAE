{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time         \n",
    "from sklearn import metrics\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchsummary import summary\n",
    "from utils.get_dataset import *\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "import datetime\n",
    "import random  \n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, hidden_dims, latent_dim):\n",
    "        super(SizeEncoder, self).__init__()\n",
    "        self.encoder = nn.ModuleList()\n",
    "        in_dim = input_dim + condition_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.encoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim),\n",
    "                    nn.ReLU())\n",
    "                    # ,nn.LayerNorm(h_dim))\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor, c: Tensor) -> List[Tensor]:\n",
    "        x = torch.cat((x, c), dim=1)\n",
    "        for module in self.encoder:\n",
    "            x = module(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "        return [mu, log_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SizeDecoder(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, condition_dim, hidden_dims, output_dim):\n",
    "        super(SizeDecoder, self).__init__()\n",
    "        self.decoder = torch.nn.ModuleList()\n",
    "        in_dim = latent_dim + condition_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.decoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim,),\n",
    "                    nn.ReLU())\n",
    "                    # nn.LayerNorm(h_dim))\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.output = nn.Linear(hidden_dims[-1], output_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor, c: Tensor) -> List[Tensor]:\n",
    "        x = torch.cat((x, c), dim=1)\n",
    "        for module in self.decoder:\n",
    "            x = module(x)\n",
    "        result = self.output(x)\n",
    "        result = F.softmax(result, dim=1)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps * std + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, dataset, optimizer):\n",
    "    dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    epoch_loss, epoch_kld, epoch_recon, sample_num, epoch_max_loss = 0, 0, 0, 0, 0\n",
    "    max_loss_weight = 0.5\n",
    "    # print(kld_weight)\n",
    "    for size_data, condition in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        size_data, condition = size_data.float().to(device), condition.float().to(device)\n",
    "        mu, var = encoder(size_data, condition)\n",
    "        z = reparameterize(mu, var)#z.shape=(batch_size,latent_dim)\n",
    "        y = decoder(z, condition)\n",
    "        recon_loss = F.l1_loss(y, size_data)\n",
    "        max_recon_loss = torch.max(torch.abs(y - size_data).mean(dim=1))\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + var - mu ** 2 - var.exp(), dim = 1), dim = 0)\n",
    "        loss = recon_loss + max_loss_weight * max_recon_loss + kld_weight * kld_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * len(size_data)\n",
    "        epoch_kld += kld_loss.item() * len(size_data)\n",
    "        epoch_recon += recon_loss.item() * len(size_data)\n",
    "        epoch_max_loss = max(epoch_max_loss, max_recon_loss.item())\n",
    "        sample_num += len(size_data)\n",
    "\n",
    "    epoch_loss /= sample_num\n",
    "    epoch_recon /= sample_num\n",
    "    epoch_kld /= sample_num\n",
    "    return epoch_loss, epoch_recon, epoch_kld, epoch_max_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(decoder,sizedata,locality,latent_dim,locality_onehots_dict,step=0, test_size=100):\n",
    "    t0=time.time()\n",
    "    # bins=20\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        condition = random.sample(locality_onehots_dict.keys(),min(len(locality_onehots_dict.keys()),500))\n",
    "        condition = [eval(i) for i in condition]\n",
    "        condition=torch.Tensor(np.array(condition)).float().to(device)\n",
    "        c_min_99 = []\n",
    "        c_min_aver = []\n",
    "        r_min_99 = []\n",
    "        r_min_aver = []\n",
    "        coverage = []\n",
    "        for cond in condition:\n",
    "            z=torch.randn([test_size,latent_dim]).to(device)\n",
    "            y=decoder(z,cond.expand(test_size,-1)).cpu()\n",
    "        # print(y.shape)#y.shape=[test_size, 类别数=65]\n",
    "            locality_key = str(list(np.array(cond.cpu())))\n",
    "            matrix=torch.zeros((test_size,len(locality_onehots_dict[locality_key])))\n",
    "            for i in range(test_size):\n",
    "                for j in range(len(locality_onehots_dict[locality_key])): \n",
    "                    matrix[i,j]=torch.max(np.abs(y[i]-sizedata[locality_onehots_dict[locality_key][j]]))   \n",
    "            # for j in range(len(dataset)):\n",
    "            #     matrix[i][j] = sum(abs(y[i]-dataset[j][0]))/dataset[j][0].shape[0]\n",
    "            column_min,column_posi=torch.min(matrix,dim=0)\n",
    "            row_min,row_posi=torch.min(matrix,dim=1)\n",
    "            c_min_99.append(np.percentile(column_min,95))\n",
    "            c_min_aver.append(np.mean(np.array(column_min)))\n",
    "            r_min_99.append(np.percentile(row_min,95))\n",
    "            r_min_aver.append(np.mean(np.array(row_min)))\n",
    "            row_posi=torch.unique(row_posi)\n",
    "            coverage.append(len(row_posi)/len(locality_onehots_dict[locality_key]))\n",
    "        # plt.hist(column_min,bins=bins,density=True,cumulative=True,color='blue')\n",
    "        # plt.hist(row_min,bins=bins,density=True,cumulative=True,color='yellow')\n",
    "        # plt.savefig('result/{date}/'.format(date=date)+str(step)+\".png\")\n",
    "\n",
    "        # print(\"coverage for testsize \"+str(test_size)+\" is :\"+str(len(row_posi)/test_size))\n",
    "        print(\"eval in\"+str(time.time()-t0)+\" //coverage is %.4f on average and is %.4f for the worst\" %\n",
    "              (np.mean(coverage), np.percentile(coverage,1)) +\n",
    "              \" //per sample error is %.4f on average and is %.4f for the worst99 and is %.4f for the worst50\" %(np.mean(r_min_aver),np.percentile(r_min_99,99),np.percentile(r_min_99,50)) +\n",
    "              \" //per source data error is %.4f on average and is %.4f for the worst99 and is %.4f for the worst50\" %(np.mean(c_min_aver),np.percentile(c_min_99,99),np.percentile(c_min_99,50)))\n",
    "        # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_locality(pairdata, freqpairs,pairsize):\n",
    "    latent_size = 128\n",
    "    \n",
    "    src_count, dst_count = defaultdict(int), defaultdict(int)\n",
    "    src_ip, dst_ip = [], []\n",
    "\n",
    "    for pair in freqpairs:\n",
    "        src_ip.append(pairdata[pair]['srcip'][0])\n",
    "        dst_ip.append(pairdata[pair]['dstip'][0])\n",
    "        src_count[pairdata[pair]['srcip'][0]] += 1\n",
    "        dst_count[pairdata[pair]['dstip'][0]] += 1\n",
    "\n",
    "    max_src_count = np.max(list(src_count.values()))\n",
    "    max_dst_count = np.max(list(dst_count.values()))\n",
    "    print(\"maxsrccount\"+str(max_src_count)+\"maxdstcount\"+str(max_dst_count))\n",
    "    condition_size = max_src_count + max_dst_count + 2\n",
    "\n",
    "    locality_strings = []\n",
    "    locality_onehots = []\n",
    "    locality_onehots_dict=defaultdict(list)\n",
    "    i = 0\n",
    "    for pair in freqpairs:\n",
    "        locality_strings.append(\"{src},{dst}\".format(src=src_count[pairdata[pair]['srcip'][0]], dst=dst_count[pairdata[pair]['dstip'][0]]))\n",
    "        locality_onehot = np.zeros(condition_size)\n",
    "        locality_onehot[src_count[pairdata[pair]['srcip'][0]]] = 1\n",
    "        locality_onehot[max_src_count + dst_count[pairdata[pair]['dstip'][0]]] = 1\n",
    "        locality_onehots.append(locality_onehot)\n",
    "        # locality_onehot.dtype=int\n",
    "        locality_onehots_dict[str(list(locality_onehot))].append(i)\n",
    "        i+=1\n",
    "    values, counts = np.unique(locality_strings, return_counts=True)\n",
    "    pair_counts = dict(zip(values, counts))\n",
    "\n",
    "    return locality_strings, np.array(locality_onehots), pair_counts, condition_size, locality_onehots_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "random.seed(114514)\n",
    "# read data\n",
    "traces = 100000\n",
    "pairdata, freqpairs, n_size, n_interval,pairsize = get_fb_data(traces)\n",
    "sizedata = get_data(pairdata, freqpairs, 'size_index', n_size)\n",
    "intervaldata = get_data(pairdata, freqpairs, 'interval_index', n_interval)\n",
    "print('read data in %dm %ds' % ((time.time() - t0) / 60, (time.time() - t0) % 60))\n",
    "\n",
    "# get locality\n",
    "locality_strings, locality_onehots, pair_counts, condition_size, locality_onehots_dict = get_locality(pairdata, freqpairs,pairsize)\n",
    "print('get locality in %dm %ds' % ((time.time() - t0) / 60, (time.time() - t0) % 60))\n",
    "dataset = [pair for pair in zip(sizedata, locality_onehots)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dims = [768, 512, 256]\n",
    "latent_dim = 32\n",
    "encoder = SizeEncoder(n_size, condition_size, hidden_dims, latent_dim).to(device)\n",
    "hidden_dims.reverse()\n",
    "decoder = SizeDecoder(latent_dim, condition_size, hidden_dims, n_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start in: 2023/11/22 02:53:18\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "kld_weight = 1e-4#1e-4不行\n",
    "optimizer = torch.optim.Adam([{'params': encoder.parameters()}, {'params': decoder.parameters()}], lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.9)\n",
    "print('start in:', time.strftime(\"%Y/%m/%d %H:%M:%S\", time.localtime()))\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.now()\n",
    "date = '%s-%s-%s-%s' % (date.year, date.month, date.day, date.hour)\n",
    "if os.path.exists('model/{date}/'.format(date=date)):\n",
    "    os.system('rm -r model/{date}/'.format(date=date))\n",
    "os.makedirs('model/{date}/'.format(date=date))\n",
    "if os.path.exists('result/{date}/'.format(date=date)):\n",
    "    os.system('rm -r result/{date}/'.format(date=date))\n",
    "os.makedirs('result/{date}/'.format(date=date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=100, avg_loss=6.19e-03, kld=20.49, recon=2.20e-03(max=1.25e-02), time=285.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2538360/4260640875.py:6: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  condition = random.sample(locality_onehots_dict.keys(),min(len(locality_onehots_dict.keys()),500))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval in48.74331259727478 //coverage is 0.7831 on average and is 0.2117 for the worst //per sample error is 0.1052 on average and is 0.3596 for the worst99 and is 0.2529 for the worst50 //per source data error is 0.0352 on average and is 0.1705 for the worst99 and is 0.0415 for the worst50\n",
      "save model\n",
      "epoch=200, avg_loss=4.29e-03, kld=13.61, recon=1.94e-03(max=6.89e-03), time=611.44\n",
      "eval in48.31687593460083 //coverage is 0.9114 on average and is 0.3157 for the worst //per sample error is 0.0666 on average and is 0.3378 for the worst99 and is 0.1346 for the worst50 //per source data error is 0.0283 on average and is 0.1638 for the worst99 and is 0.0322 for the worst50\n",
      "save model\n",
      "epoch=300, avg_loss=3.89e-03, kld=11.61, recon=1.87e-03(max=5.97e-03), time=935.80\n",
      "eval in45.818854331970215 //coverage is 0.9198 on average and is 0.2498 for the worst //per sample error is 0.0594 on average and is 0.3067 for the worst99 and is 0.0971 for the worst50 //per source data error is 0.0284 on average and is 0.2103 for the worst99 and is 0.0295 for the worst50\n",
      "save model\n",
      "epoch=400, avg_loss=3.78e-03, kld=11.70, recon=1.79e-03(max=4.57e-03), time=1258.97\n",
      "eval in52.27905988693237 //coverage is 0.9123 on average and is 0.2497 for the worst //per sample error is 0.0507 on average and is 0.3262 for the worst99 and is 0.0728 for the worst50 //per source data error is 0.0291 on average and is 0.3125 for the worst99 and is 0.0299 for the worst50\n",
      "save model\n",
      "epoch=500, avg_loss=3.72e-03, kld=11.10, recon=1.77e-03(max=5.16e-03), time=1590.00\n",
      "eval in48.1550612449646 //coverage is 0.9262 on average and is 0.4636 for the worst //per sample error is 0.0490 on average and is 0.3540 for the worst99 and is 0.0705 for the worst50 //per source data error is 0.0264 on average and is 0.2192 for the worst99 and is 0.0284 for the worst50\n",
      "save model\n",
      "epoch=600, avg_loss=3.74e-03, kld=10.50, recon=1.77e-03(max=4.03e-03), time=1916.77\n",
      "eval in52.78867244720459 //coverage is 0.9225 on average and is 0.5000 for the worst //per sample error is 0.0439 on average and is 0.3277 for the worst99 and is 0.0629 for the worst50 //per source data error is 0.0245 on average and is 0.2335 for the worst99 and is 0.0297 for the worst50\n",
      "epoch=700, avg_loss=3.79e-03, kld=10.09, recon=1.77e-03(max=3.72e-03), time=2246.92\n",
      "eval in47.61828374862671 //coverage is 0.9305 on average and is 0.4999 for the worst //per sample error is 0.0446 on average and is 0.3084 for the worst99 and is 0.0649 for the worst50 //per source data error is 0.0249 on average and is 0.2411 for the worst99 and is 0.0286 for the worst50\n",
      "epoch=800, avg_loss=3.85e-03, kld=9.51, recon=1.78e-03(max=5.07e-03), time=2571.76\n",
      "eval in45.42648506164551 //coverage is 0.9372 on average and is 0.5542 for the worst //per sample error is 0.0431 on average and is 0.2910 for the worst99 and is 0.0656 for the worst50 //per source data error is 0.0246 on average and is 0.2316 for the worst99 and is 0.0277 for the worst50\n",
      "epoch=900, avg_loss=3.91e-03, kld=9.07, recon=1.79e-03(max=4.51e-03), time=2894.09\n",
      "eval in44.64872932434082 //coverage is 0.9359 on average and is 0.5555 for the worst //per sample error is 0.0440 on average and is 0.3162 for the worst99 and is 0.0647 for the worst50 //per source data error is 0.0231 on average and is 0.2119 for the worst99 and is 0.0275 for the worst50\n",
      "epoch=1000, avg_loss=3.97e-03, kld=8.66, recon=1.80e-03(max=4.74e-03), time=3203.66\n",
      "eval in359.62763595581055 //coverage is 0.9942 on average and is 0.9165 for the worst //per sample error is 0.0424 on average and is 0.3167 for the worst99 and is 0.0625 for the worst50 //per source data error is 0.0168 on average and is 0.0878 for the worst99 and is 0.0210 for the worst50\n",
      "epoch=1100, avg_loss=3.99e-03, kld=8.65, recon=1.79e-03(max=4.39e-03), time=3838.23\n",
      "eval in47.597235918045044 //coverage is 0.9356 on average and is 0.5782 for the worst //per sample error is 0.0421 on average and is 0.3050 for the worst99 and is 0.0610 for the worst50 //per source data error is 0.0227 on average and is 0.2342 for the worst99 and is 0.0281 for the worst50\n",
      "epoch=1200, avg_loss=3.96e-03, kld=8.70, recon=1.78e-03(max=3.58e-03), time=4160.50\n",
      "eval in44.45399808883667 //coverage is 0.9341 on average and is 0.5897 for the worst //per sample error is 0.0416 on average and is 0.2851 for the worst99 and is 0.0617 for the worst50 //per source data error is 0.0232 on average and is 0.2364 for the worst99 and is 0.0274 for the worst50\n",
      "epoch=1300, avg_loss=3.94e-03, kld=8.71, recon=1.77e-03(max=8.34e-03), time=4478.21\n",
      "eval in49.14109992980957 //coverage is 0.9258 on average and is 0.5313 for the worst //per sample error is 0.0383 on average and is 0.2724 for the worst99 and is 0.0582 for the worst50 //per source data error is 0.0229 on average and is 0.2401 for the worst99 and is 0.0274 for the worst50\n",
      "epoch=1400, avg_loss=3.92e-03, kld=8.79, recon=1.76e-03(max=3.86e-03), time=4800.60\n",
      "eval in48.876821517944336 //coverage is 0.9286 on average and is 0.5441 for the worst //per sample error is 0.0377 on average and is 0.1881 for the worst99 and is 0.0586 for the worst50 //per source data error is 0.0225 on average and is 0.1999 for the worst99 and is 0.0284 for the worst50\n",
      "epoch=1500, avg_loss=3.90e-03, kld=8.79, recon=1.75e-03(max=4.31e-03), time=5124.04\n",
      "eval in44.32116746902466 //coverage is 0.9363 on average and is 0.5796 for the worst //per sample error is 0.0396 on average and is 0.2527 for the worst99 and is 0.0607 for the worst50 //per source data error is 0.0224 on average and is 0.2156 for the worst99 and is 0.0269 for the worst50\n",
      "epoch=1600, avg_loss=3.88e-03, kld=8.93, recon=1.73e-03(max=4.63e-03), time=5445.08\n",
      "eval in47.248162508010864 //coverage is 0.9353 on average and is 0.5305 for the worst //per sample error is 0.0367 on average and is 0.1428 for the worst99 and is 0.0581 for the worst50 //per source data error is 0.0212 on average and is 0.2178 for the worst99 and is 0.0269 for the worst50\n",
      "epoch=1700, avg_loss=3.87e-03, kld=8.95, recon=1.72e-03(max=3.53e-03), time=5766.97\n",
      "eval in45.14442706108093 //coverage is 0.9387 on average and is 0.5925 for the worst //per sample error is 0.0399 on average and is 0.2523 for the worst99 and is 0.0602 for the worst50 //per source data error is 0.0220 on average and is 0.1963 for the worst99 and is 0.0270 for the worst50\n",
      "epoch=1800, avg_loss=3.85e-03, kld=9.13, recon=1.71e-03(max=3.96e-03), time=6086.37\n",
      "eval in45.117021799087524 //coverage is 0.9377 on average and is 0.5810 for the worst //per sample error is 0.0390 on average and is 0.2166 for the worst99 and is 0.0608 for the worst50 //per source data error is 0.0221 on average and is 0.2227 for the worst99 and is 0.0267 for the worst50\n",
      "epoch=1900, avg_loss=3.84e-03, kld=9.00, recon=1.71e-03(max=1.08e-02), time=6405.74\n",
      "eval in43.798677921295166 //coverage is 0.9387 on average and is 0.6065 for the worst //per sample error is 0.0382 on average and is 0.2289 for the worst99 and is 0.0586 for the worst50 //per source data error is 0.0222 on average and is 0.2268 for the worst99 and is 0.0270 for the worst50\n",
      "epoch=2000, avg_loss=3.82e-03, kld=9.13, recon=1.70e-03(max=5.25e-03), time=6710.29\n",
      "eval in342.18186473846436 //coverage is 0.9928 on average and is 0.8748 for the worst //per sample error is 0.0379 on average and is 0.2183 for the worst99 and is 0.0595 for the worst50 //per source data error is 0.0155 on average and is 0.0869 for the worst99 and is 0.0203 for the worst50\n",
      "epoch=2100, avg_loss=2.56e-03, kld=17.95, recon=1.42e-03(max=3.85e-03), time=7326.77\n",
      "eval in46.71263074874878 //coverage is 0.9309 on average and is 0.5454 for the worst //per sample error is 0.0377 on average and is 0.1395 for the worst99 and is 0.0581 for the worst50 //per source data error is 0.0220 on average and is 0.2147 for the worst99 and is 0.0270 for the worst50\n",
      "save model\n",
      "epoch=2200, avg_loss=2.69e-03, kld=16.14, recon=1.42e-03(max=4.47e-03), time=7650.77\n",
      "eval in51.18538212776184 //coverage is 0.9292 on average and is 0.5555 for the worst //per sample error is 0.0381 on average and is 0.2339 for the worst99 and is 0.0568 for the worst50 //per source data error is 0.0224 on average and is 0.2288 for the worst99 and is 0.0272 for the worst50\n",
      "epoch=2300, avg_loss=2.82e-03, kld=15.09, recon=1.42e-03(max=2.62e-03), time=7979.78\n",
      "eval in49.99188828468323 //coverage is 0.9269 on average and is 0.5968 for the worst //per sample error is 0.0405 on average and is 0.2510 for the worst99 and is 0.0602 for the worst50 //per source data error is 0.0224 on average and is 0.2470 for the worst99 and is 0.0281 for the worst50\n",
      "epoch=2400, avg_loss=2.95e-03, kld=13.91, recon=1.44e-03(max=3.88e-03), time=8306.04\n",
      "eval in51.41916108131409 //coverage is 0.9268 on average and is 0.5326 for the worst //per sample error is 0.0412 on average and is 0.2577 for the worst99 and is 0.0591 for the worst50 //per source data error is 0.0230 on average and is 0.2209 for the worst99 and is 0.0287 for the worst50\n",
      "epoch=2500, avg_loss=3.08e-03, kld=13.06, recon=1.46e-03(max=3.53e-03), time=8634.34\n",
      "eval in45.08263373374939 //coverage is 0.9427 on average and is 0.5600 for the worst //per sample error is 0.0435 on average and is 0.3514 for the worst99 and is 0.0606 for the worst50 //per source data error is 0.0231 on average and is 0.2098 for the worst99 and is 0.0275 for the worst50\n",
      "epoch=2600, avg_loss=3.21e-03, kld=12.48, recon=1.47e-03(max=2.74e-03), time=8955.49\n",
      "eval in47.573298931121826 //coverage is 0.9316 on average and is 0.6029 for the worst //per sample error is 0.0384 on average and is 0.2299 for the worst99 and is 0.0585 for the worst50 //per source data error is 0.0222 on average and is 0.2161 for the worst99 and is 0.0274 for the worst50\n",
      "epoch=2700, avg_loss=3.33e-03, kld=11.83, recon=1.50e-03(max=4.05e-03), time=9278.71\n",
      "eval in45.87861490249634 //coverage is 0.9386 on average and is 0.5977 for the worst //per sample error is 0.0417 on average and is 0.2897 for the worst99 and is 0.0601 for the worst50 //per source data error is 0.0234 on average and is 0.2274 for the worst99 and is 0.0269 for the worst50\n",
      "epoch=2800, avg_loss=3.45e-03, kld=11.09, recon=1.53e-03(max=4.64e-03), time=9599.40\n",
      "eval in44.80749201774597 //coverage is 0.9440 on average and is 0.6000 for the worst //per sample error is 0.0389 on average and is 0.2201 for the worst99 and is 0.0611 for the worst50 //per source data error is 0.0211 on average and is 0.2136 for the worst99 and is 0.0268 for the worst50\n",
      "epoch=2900, avg_loss=6.02e+01, kld=10.49, recon=1.55e-03(max=4.80e-03), time=9920.87\n",
      "eval in46.55943059921265 //coverage is 0.9384 on average and is 0.6338 for the worst //per sample error is 0.0392 on average and is 0.2385 for the worst99 and is 0.0593 for the worst50 //per source data error is 0.0219 on average and is 0.2121 for the worst99 and is 0.0268 for the worst50\n",
      "epoch=3000, avg_loss=3.66e-03, kld=9.95, recon=1.58e-03(max=6.86e-03), time=10232.00\n",
      "eval in389.04035782814026 //coverage is 0.9938 on average and is 0.8998 for the worst //per sample error is 0.0383 on average and is 0.2436 for the worst99 and is 0.0576 for the worst50 //per source data error is 0.0152 on average and is 0.0781 for the worst99 and is 0.0195 for the worst50\n",
      "epoch=3100, avg_loss=3.71e-03, kld=9.98, recon=1.57e-03(max=2.75e-03), time=10895.19\n",
      "eval in45.999835729599 //coverage is 0.9390 on average and is 0.5926 for the worst //per sample error is 0.0383 on average and is 0.2308 for the worst99 and is 0.0585 for the worst50 //per source data error is 0.0219 on average and is 0.2147 for the worst99 and is 0.0272 for the worst50\n",
      "epoch=3200, avg_loss=3.70e-03, kld=9.98, recon=1.58e-03(max=4.58e-03), time=11215.96\n",
      "eval in45.940680742263794 //coverage is 0.9376 on average and is 0.5675 for the worst //per sample error is 0.0400 on average and is 0.2532 for the worst99 and is 0.0592 for the worst50 //per source data error is 0.0222 on average and is 0.2186 for the worst99 and is 0.0267 for the worst50\n",
      "epoch=3300, avg_loss=3.70e-03, kld=9.99, recon=1.57e-03(max=1.27e-02), time=11534.85\n",
      "eval in48.26739001274109 //coverage is 0.9320 on average and is 0.5831 for the worst //per sample error is 0.0404 on average and is 0.2556 for the worst99 and is 0.0587 for the worst50 //per source data error is 0.0226 on average and is 0.2426 for the worst99 and is 0.0270 for the worst50\n",
      "epoch=3400, avg_loss=3.69e-03, kld=9.85, recon=1.57e-03(max=3.44e-03), time=11858.46\n",
      "eval in47.2926082611084 //coverage is 0.9373 on average and is 0.6049 for the worst //per sample error is 0.0389 on average and is 0.2768 for the worst99 and is 0.0588 for the worst50 //per source data error is 0.0217 on average and is 0.2079 for the worst99 and is 0.0278 for the worst50\n",
      "epoch=3500, avg_loss=3.72e-03, kld=9.97, recon=1.56e-03(max=4.94e-03), time=12180.21\n",
      "eval in46.67661118507385 //coverage is 0.9367 on average and is 0.5651 for the worst //per sample error is 0.0393 on average and is 0.2499 for the worst99 and is 0.0580 for the worst50 //per source data error is 0.0214 on average and is 0.1955 for the worst99 and is 0.0265 for the worst50\n",
      "epoch=3600, avg_loss=9.57e-01, kld=9.90, recon=1.56e-03(max=2.94e-03), time=12501.18\n",
      "eval in47.053075313568115 //coverage is 0.9345 on average and is 0.5598 for the worst //per sample error is 0.0377 on average and is 0.2000 for the worst99 and is 0.0569 for the worst50 //per source data error is 0.0219 on average and is 0.2150 for the worst99 and is 0.0276 for the worst50\n",
      "epoch=3700, avg_loss=3.67e-03, kld=9.85, recon=1.56e-03(max=6.39e-03), time=12822.79\n",
      "eval in48.38092303276062 //coverage is 0.9335 on average and is 0.5897 for the worst //per sample error is 0.0389 on average and is 0.1946 for the worst99 and is 0.0591 for the worst50 //per source data error is 0.0224 on average and is 0.2212 for the worst99 and is 0.0271 for the worst50\n",
      "epoch=3800, avg_loss=3.67e-03, kld=9.87, recon=1.56e-03(max=3.09e-03), time=13146.45\n",
      "eval in48.578327894210815 //coverage is 0.9385 on average and is 0.5306 for the worst //per sample error is 0.0368 on average and is 0.1773 for the worst99 and is 0.0569 for the worst50 //per source data error is 0.0208 on average and is 0.2202 for the worst99 and is 0.0272 for the worst50\n",
      "epoch=3900, avg_loss=7.15e-02, kld=9.93, recon=1.56e-03(max=2.98e-03), time=13470.06\n",
      "eval in43.799490213394165 //coverage is 0.9293 on average and is 0.5769 for the worst //per sample error is 0.0377 on average and is 0.2464 for the worst99 and is 0.0562 for the worst50 //per source data error is 0.0219 on average and is 0.2141 for the worst99 and is 0.0280 for the worst50\n",
      "epoch=4000, avg_loss=nan, kld=nan, recon=nan(max=0.00e+00), time=13769.82\n",
      "eval in394.81100058555603 //coverage is 0.3002 on average and is 0.0098 for the worst //per sample error is nan on average and is nan for the worst99 and is nan for the worst50 //per source data error is nan on average and is nan for the worst99 and is nan for the worst50\n",
      "epoch=4100, avg_loss=nan, kld=nan, recon=nan(max=0.00e+00), time=14439.43\n",
      "eval in46.22802686691284 //coverage is 0.3038 on average and is 0.0128 for the worst //per sample error is nan on average and is nan for the worst99 and is nan for the worst50 //per source data error is nan on average and is nan for the worst99 and is nan for the worst50\n",
      "epoch=4200, avg_loss=nan, kld=nan, recon=nan(max=0.00e+00), time=14760.45\n",
      "eval in50.881569385528564 //coverage is 0.2823 on average and is 0.0137 for the worst //per sample error is nan on average and is nan for the worst99 and is nan for the worst50 //per source data error is nan on average and is nan for the worst99 and is nan for the worst50\n",
      "epoch=4300, avg_loss=nan, kld=nan, recon=nan(max=0.00e+00), time=15089.51\n",
      "eval in47.436479330062866 //coverage is 0.2940 on average and is 0.0116 for the worst //per sample error is nan on average and is nan for the worst99 and is nan for the worst50 //per source data error is nan on average and is nan for the worst99 and is nan for the worst50\n",
      "epoch=4400, avg_loss=nan, kld=nan, recon=nan(max=0.00e+00), time=15414.84\n",
      "eval in47.429959297180176 //coverage is 0.2774 on average and is 0.0132 for the worst //per sample error is nan on average and is nan for the worst99 and is nan for the worst50 //per source data error is nan on average and is nan for the worst99 and is nan for the worst50\n",
      "epoch=4500, avg_loss=nan, kld=nan, recon=nan(max=0.00e+00), time=15738.31\n",
      "eval in46.923569679260254 //coverage is 0.2950 on average and is 0.0127 for the worst //per sample error is nan on average and is nan for the worst99 and is nan for the worst50 //per source data error is nan on average and is nan for the worst99 and is nan for the worst50\n",
      "epoch=4600, avg_loss=nan, kld=nan, recon=nan(max=0.00e+00), time=16068.57\n",
      "eval in50.88667035102844 //coverage is 0.2978 on average and is 0.0123 for the worst //per sample error is nan on average and is nan for the worst99 and is nan for the worst50 //per source data error is nan on average and is nan for the worst99 and is nan for the worst50\n",
      "epoch=4700, avg_loss=nan, kld=nan, recon=nan(max=0.00e+00), time=16400.06\n",
      "eval in46.23387551307678 //coverage is 0.3279 on average and is 0.0128 for the worst //per sample error is nan on average and is nan for the worst99 and is nan for the worst50 //per source data error is nan on average and is nan for the worst99 and is nan for the worst50\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "avg_loss = 0\n",
    "min_loss = 1e9\n",
    "print_every = 100\n",
    "for epoch in range(100001):\n",
    "    kld_weight = get_kld_weight(epoch)\n",
    "    epoch_loss, epoch_recon, epoch_kld, max_loss = train(encoder, decoder, dataset, optimizer)\n",
    "    avg_loss += epoch_loss\n",
    "    if epoch and epoch % print_every == 0:\n",
    "        avg_loss /= print_every\n",
    "        cur_time = time.time()\n",
    "        print(\"epoch=%d, avg_loss=%.2e, kld=%.2f, recon=%.2e(max=%.2e), time=%.2f\" % (epoch, avg_loss, epoch_kld, epoch_recon, max_loss, cur_time - start_time))\n",
    "        if epoch % (print_every*10) == 0:\n",
    "            evaluate(decoder,sizedata,locality_onehots,latent_dim,locality_onehots_dict,epoch,1000)\n",
    "        else:\n",
    "            evaluate(decoder,sizedata,locality_onehots,latent_dim,locality_onehots_dict,epoch,100)\n",
    "        if avg_loss < min_loss:\n",
    "            min_loss = avg_loss\n",
    "            torch.save(encoder, 'model/{date}/encoder.pth'.format(date=date))\n",
    "            torch.save(decoder, 'model/{date}/decoder.pth'.format(date=date))\n",
    "            print('save model')\n",
    "        sys.stdout.flush()\n",
    "        if avg_loss < 1e-3:\n",
    "            break\n",
    "        avg_loss = 0\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
