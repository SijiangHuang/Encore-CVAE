{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time         \n",
    "from sklearn import metrics\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchsummary import summary\n",
    "from utils.get_dataset import *\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "import datetime\n",
    "import random  \n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, hidden_dims, latent_dim):\n",
    "        super(SizeEncoder, self).__init__()\n",
    "        self.encoder = nn.ModuleList()\n",
    "        in_dim = input_dim + condition_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.encoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim),\n",
    "                    nn.ReLU())\n",
    "                    # ,nn.LayerNorm(h_dim))\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor, c: Tensor) -> List[Tensor]:\n",
    "        x = torch.cat((x, c), dim=1)\n",
    "        for module in self.encoder:\n",
    "            x = module(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "        return [mu, log_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SizeDecoder(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, condition_dim, hidden_dims, output_dim):\n",
    "        super(SizeDecoder, self).__init__()\n",
    "        self.decoder = torch.nn.ModuleList()\n",
    "        in_dim = latent_dim + condition_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.decoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim,),\n",
    "                    nn.ReLU())\n",
    "                    # nn.LayerNorm(h_dim))\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.output = nn.Linear(hidden_dims[-1], output_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor, c: Tensor) -> List[Tensor]:\n",
    "        x = torch.cat((x, c), dim=1)\n",
    "        for module in self.decoder:\n",
    "            x = module(x)\n",
    "        result = self.output(x)\n",
    "        result = F.softmax(result, dim=1)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps * std + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, dataset, optimizer):\n",
    "    dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    epoch_loss, epoch_kld, epoch_recon, sample_num, epoch_max_loss = 0, 0, 0, 0, 0\n",
    "    max_loss_weight = 0.5\n",
    "    # print(kld_weight)\n",
    "    for size_data, condition in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        size_data, condition = size_data.float().to(device), condition.float().to(device)\n",
    "        mu, var = encoder(size_data, condition)\n",
    "        z = reparameterize(mu, var)#z.shape=(batch_size,latent_dim)\n",
    "        y = decoder(z, condition)\n",
    "        recon_loss = F.l1_loss(y, size_data)\n",
    "        max_recon_loss = torch.max(torch.abs(y - size_data).mean(dim=1))\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + var - mu ** 2 - var.exp(), dim = 1), dim = 0)\n",
    "        loss = recon_loss + max_loss_weight * max_recon_loss + kld_weight * kld_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * len(size_data)\n",
    "        epoch_kld += kld_loss.item() * len(size_data)\n",
    "        epoch_recon += recon_loss.item() * len(size_data)\n",
    "        epoch_max_loss = max(epoch_max_loss, max_recon_loss.item())\n",
    "        sample_num += len(size_data)\n",
    "\n",
    "    epoch_loss /= sample_num\n",
    "    epoch_recon /= sample_num\n",
    "    epoch_kld /= sample_num\n",
    "    return epoch_loss, epoch_recon, epoch_kld, epoch_max_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(decoder,sizedata,locality,latent_dim,locality_onehots_dict,step=0, test_size=100):\n",
    "    t0=time.time()\n",
    "    # bins=20\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        condition = random.sample(locality_onehots_dict.keys(),min(len(locality_onehots_dict.keys()),500))\n",
    "        condition = [eval(i) for i in condition]\n",
    "        condition=torch.Tensor(np.array(condition)).float().to(device)\n",
    "        c_min_99 = []\n",
    "        c_min_aver = []\n",
    "        r_min_99 = []\n",
    "        r_min_aver = []\n",
    "        coverage = []\n",
    "        for cond in condition:\n",
    "            z=torch.randn([test_size,latent_dim]).to(device)\n",
    "            y=decoder(z,cond.expand(test_size,-1)).cpu()\n",
    "        # print(y.shape)#y.shape=[test_size, 类别数=65]\n",
    "            locality_key = str(list(np.array(cond.cpu())))\n",
    "            matrix=torch.zeros((test_size,len(locality_onehots_dict[locality_key])))\n",
    "            for i in range(test_size):\n",
    "                for j in range(len(locality_onehots_dict[locality_key])): \n",
    "                    matrix[i,j]=torch.max(np.abs(y[i]-sizedata[locality_onehots_dict[locality_key][j]]))   \n",
    "            # for j in range(len(dataset)):\n",
    "            #     matrix[i][j] = sum(abs(y[i]-dataset[j][0]))/dataset[j][0].shape[0]\n",
    "            column_min,column_posi=torch.min(matrix,dim=0)\n",
    "            row_min,row_posi=torch.min(matrix,dim=1)\n",
    "            c_min_99.append(np.percentile(column_min,95))\n",
    "            c_min_aver.append(np.mean(np.array(column_min)))\n",
    "            r_min_99.append(np.percentile(row_min,95))\n",
    "            r_min_aver.append(np.mean(np.array(row_min)))\n",
    "            row_posi=torch.unique(row_posi)\n",
    "            coverage.append(len(row_posi)/len(locality_onehots_dict[locality_key]))\n",
    "        # plt.hist(column_min,bins=bins,density=True,cumulative=True,color='blue')\n",
    "        # plt.hist(row_min,bins=bins,density=True,cumulative=True,color='yellow')\n",
    "        # plt.savefig('result/{date}/'.format(date=date)+str(step)+\".png\")\n",
    "\n",
    "        # print(\"coverage for testsize \"+str(test_size)+\" is :\"+str(len(row_posi)/test_size))\n",
    "        print(\"eval in\"+str(time.time()-t0)+\" //coverage is %.4f on average and is %.4f for the worst\" %\n",
    "              (np.mean(coverage), np.percentile(coverage,1)) +\n",
    "              \" //per sample error is %.4f on average and is %.4f for the worst99 and is %.4f for the worst50\" %(np.mean(r_min_aver),np.percentile(r_min_99,99),np.percentile(r_min_99,50)) +\n",
    "              \" //per source data error is %.4f on average and is %.4f for the worst99 and is %.4f for the worst50\" %(np.mean(c_min_aver),np.percentile(c_min_99,99),np.percentile(c_min_99,50)))\n",
    "        # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_locality(pairdata, freqpairs,pairsize):\n",
    "    latent_size = 128\n",
    "    \n",
    "    src_count, dst_count = defaultdict(int), defaultdict(int)\n",
    "    src_ip, dst_ip = [], []\n",
    "\n",
    "    for pair in freqpairs:\n",
    "        src_ip.append(pairdata[pair]['srcip'][0])\n",
    "        dst_ip.append(pairdata[pair]['dstip'][0])\n",
    "        src_count[pairdata[pair]['srcip'][0]] += 1\n",
    "        dst_count[pairdata[pair]['dstip'][0]] += 1\n",
    "\n",
    "    max_src_count = np.max(list(src_count.values()))\n",
    "    max_dst_count = np.max(list(dst_count.values()))\n",
    "    print(\"maxsrccount\"+str(max_src_count)+\"maxdstcount\"+str(max_dst_count))\n",
    "    condition_size = max_src_count + max_dst_count + 2\n",
    "\n",
    "    locality_strings = []\n",
    "    locality_onehots = []\n",
    "    locality_onehots_dict=defaultdict(list)\n",
    "    i = 0\n",
    "    for pair in freqpairs:\n",
    "        locality_strings.append(\"{src},{dst}\".format(src=src_count[pairdata[pair]['srcip'][0]], dst=dst_count[pairdata[pair]['dstip'][0]]))\n",
    "        locality_onehot = np.zeros(condition_size)\n",
    "        locality_onehot[src_count[pairdata[pair]['srcip'][0]]] = 1\n",
    "        locality_onehot[max_src_count + dst_count[pairdata[pair]['dstip'][0]]] = 1\n",
    "        locality_onehots.append(locality_onehot)\n",
    "        # locality_onehot.dtype=int\n",
    "        locality_onehots_dict[str(list(locality_onehot))].append(i)\n",
    "        i+=1\n",
    "    values, counts = np.unique(locality_strings, return_counts=True)\n",
    "    pair_counts = dict(zip(values, counts))\n",
    "\n",
    "    return locality_strings, np.array(locality_onehots), pair_counts, condition_size, locality_onehots_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data in 4m 19s\n",
      "maxsrccount106maxdstcount129\n",
      "get locality in 4m 32s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "random.seed(114514)\n",
    "# read data\n",
    "traces = 100000\n",
    "pairdata, freqpairs, n_size, n_interval,pairsize = get_fb_data(traces)\n",
    "sizedata = get_data(pairdata, freqpairs, 'size_index', n_size)\n",
    "intervaldata = get_data(pairdata, freqpairs, 'interval_index', n_interval)\n",
    "print('read data in %dm %ds' % ((time.time() - t0) / 60, (time.time() - t0) % 60))\n",
    "\n",
    "# get locality\n",
    "locality_strings, locality_onehots, pair_counts, condition_size, locality_onehots_dict = get_locality(pairdata, freqpairs,pairsize)\n",
    "print('get locality in %dm %ds' % ((time.time() - t0) / 60, (time.time() - t0) % 60))\n",
    "dataset = [pair for pair in zip(sizedata, locality_onehots)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dims = [768, 512, 256]\n",
    "latent_dim = 32\n",
    "encoder = SizeEncoder(n_size, condition_size, hidden_dims, latent_dim).to(device)\n",
    "hidden_dims.reverse()\n",
    "decoder = SizeDecoder(latent_dim, condition_size, hidden_dims, n_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start in: 2023/11/22 07:40:44\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "kld_weight = 1e-4#1e-4不行\n",
    "optimizer = torch.optim.Adam([{'params': encoder.parameters()}, {'params': decoder.parameters()}], lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.9)\n",
    "print('start in:', time.strftime(\"%Y/%m/%d %H:%M:%S\", time.localtime()))\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.now()\n",
    "date = '%s-%s-%s-%s' % (date.year, date.month, date.day, date.hour)\n",
    "if os.path.exists('model/{date}/'.format(date=date)):\n",
    "    os.system('rm -r model/{date}/'.format(date=date))\n",
    "os.makedirs('model/{date}/'.format(date=date))\n",
    "if os.path.exists('result/{date}/'.format(date=date)):\n",
    "    os.system('rm -r result/{date}/'.format(date=date))\n",
    "os.makedirs('result/{date}/'.format(date=date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kld_weight(epoch=0):\n",
    "    kld_max=1e-4\n",
    "    kld_min=1e-6\n",
    "    if epoch<1000:\n",
    "        return (kld_max-kld_min)*((epoch)/1000.0)+kld_min\n",
    "    else:\n",
    "        return kld_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=100, avg_loss=nan, kld=nan, recon=nan(max=0.00e+00), time=510.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3102201/4260640875.py:6: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  condition = random.sample(locality_onehots_dict.keys(),min(len(locality_onehots_dict.keys()),500))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval in46.665637493133545 //coverage is 0.2950 on average and is 0.0127 for the worst //per sample error is nan on average and is nan for the worst99 and is nan for the worst50 //per source data error is nan on average and is nan for the worst99 and is nan for the worst50\n",
      "epoch=200, avg_loss=nan, kld=nan, recon=nan(max=0.00e+00), time=829.58\n",
      "eval in50.23838663101196 //coverage is 0.2978 on average and is 0.0123 for the worst //per sample error is nan on average and is nan for the worst99 and is nan for the worst50 //per source data error is nan on average and is nan for the worst99 and is nan for the worst50\n",
      "epoch=300, avg_loss=nan, kld=nan, recon=nan(max=0.00e+00), time=1264.80\n",
      "eval in85.78566002845764 //coverage is 0.3279 on average and is 0.0128 for the worst //per sample error is nan on average and is nan for the worst99 and is nan for the worst50 //per source data error is nan on average and is nan for the worst99 and is nan for the worst50\n",
      "epoch=400, avg_loss=nan, kld=nan, recon=nan(max=0.00e+00), time=1819.21\n",
      "eval in84.83345937728882 //coverage is 0.3035 on average and is 0.0116 for the worst //per sample error is nan on average and is nan for the worst99 and is nan for the worst50 //per source data error is nan on average and is nan for the worst99 and is nan for the worst50\n",
      "epoch=500, avg_loss=nan, kld=nan, recon=nan(max=0.00e+00), time=2333.56\n",
      "eval in82.19171929359436 //coverage is 0.3084 on average and is 0.0139 for the worst //per sample error is nan on average and is nan for the worst99 and is nan for the worst50 //per source data error is nan on average and is nan for the worst99 and is nan for the worst50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/Encore-CVAE/size_cvae.ipynb 单元格 13\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100001\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     kld_weight \u001b[39m=\u001b[39m get_kld_weight(epoch)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     epoch_loss, epoch_recon, epoch_kld, max_loss \u001b[39m=\u001b[39m train(encoder, decoder, dataset, optimizer)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m epoch_loss\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39mand\u001b[39;00m epoch \u001b[39m%\u001b[39m print_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/root/Encore-CVAE/size_cvae.ipynb 单元格 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m kld_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(\u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m var \u001b[39m-\u001b[39m mu \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m var\u001b[39m.\u001b[39mexp(), dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m), dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m recon_loss \u001b[39m+\u001b[39m max_loss_weight \u001b[39m*\u001b[39m max_recon_loss \u001b[39m+\u001b[39m kld_weight \u001b[39m*\u001b[39m kld_loss\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(size_data)\n",
      "File \u001b[0;32m~/anaconda3/envs/plf/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/plf/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "avg_loss = 0\n",
    "min_loss = 1e9\n",
    "print_every = 100\n",
    "for epoch in range(100001):\n",
    "    kld_weight = get_kld_weight(epoch)\n",
    "    epoch_loss, epoch_recon, epoch_kld, max_loss = train(encoder, decoder, dataset, optimizer)\n",
    "    avg_loss += epoch_loss\n",
    "    if epoch and epoch % print_every == 0:\n",
    "        avg_loss /= print_every\n",
    "        cur_time = time.time()\n",
    "        print(\"epoch=%d, avg_loss=%.2e, kld=%.2f, recon=%.2e(max=%.2e), time=%.2f\" % (epoch, avg_loss, epoch_kld, epoch_recon, max_loss, cur_time - start_time))\n",
    "        if epoch % (print_every*10) == 0:\n",
    "            evaluate(decoder,sizedata,locality_onehots,latent_dim,locality_onehots_dict,epoch,1000)\n",
    "        else:\n",
    "            evaluate(decoder,sizedata,locality_onehots,latent_dim,locality_onehots_dict,epoch,100)\n",
    "        if avg_loss < min_loss:\n",
    "            min_loss = avg_loss\n",
    "            torch.save(encoder, 'model/{date}/encoder.pth'.format(date=date))\n",
    "            torch.save(decoder, 'model/{date}/decoder.pth'.format(date=date))\n",
    "            print('save model')\n",
    "        sys.stdout.flush()\n",
    "        if avg_loss < 1e-3:\n",
    "            break\n",
    "        avg_loss = 0\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
