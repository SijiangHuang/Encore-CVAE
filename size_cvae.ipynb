{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time         \n",
    "from sklearn import metrics\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchsummary import summary\n",
    "from utils.get_dataset import *\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "import datetime\n",
    "import random  \n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, hidden_dims, latent_dim):\n",
    "        super(SizeEncoder, self).__init__()\n",
    "        self.encoder = nn.ModuleList()\n",
    "        in_dim = input_dim + condition_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.encoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim),\n",
    "                    nn.ReLU())\n",
    "                    # ,nn.LayerNorm(h_dim))\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor, c: Tensor) -> List[Tensor]:\n",
    "        x = torch.cat((x, c), dim=1)\n",
    "        for module in self.encoder:\n",
    "            x = module(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "        return [mu, log_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SizeDecoder(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, condition_dim, hidden_dims, output_dim):\n",
    "        super(SizeDecoder, self).__init__()\n",
    "        self.decoder = torch.nn.ModuleList()\n",
    "        in_dim = latent_dim + condition_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.decoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim,),\n",
    "                    nn.ReLU())\n",
    "                    # nn.LayerNorm(h_dim))\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.output = nn.Linear(hidden_dims[-1], output_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor, c: Tensor) -> List[Tensor]:\n",
    "        x = torch.cat((x, c), dim=1)\n",
    "        for module in self.decoder:\n",
    "            x = module(x)\n",
    "        result = self.output(x)\n",
    "        result = F.softmax(result, dim=1)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps * std + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, dataset, optimizer):\n",
    "    dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    epoch_loss, epoch_kld, epoch_recon, sample_num, epoch_max_loss = 0, 0, 0, 0, 0\n",
    "    max_loss_weight = 0.5\n",
    "    # print(kld_weight)\n",
    "    for size_data, condition in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        size_data, condition = size_data.float().to(device), condition.float().to(device)\n",
    "        mu, var = encoder(size_data, condition)\n",
    "        z = reparameterize(mu, var)#z.shape=(batch_size,latent_dim)\n",
    "        y = decoder(z, condition)\n",
    "        recon_loss = F.l1_loss(y, size_data)\n",
    "        max_recon_loss = torch.max(torch.abs(y - size_data).mean(dim=1))\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + var - mu ** 2 - var.exp(), dim = 1), dim = 0)\n",
    "        loss = recon_loss + max_loss_weight * max_recon_loss + kld_weight * kld_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if loss.item() * len(size_data) > 200:\n",
    "            print(\"loss \"+str(loss.item() * len(size_data))+\" kld_loss\"+str(kld_loss.item() * len(size_data))+\n",
    "                  \" reconloss\"+str(recon_loss.item() * len(size_data))+\" max_reconloss\"+str(max_recon_loss.item()))\n",
    "        epoch_loss += loss.item() * len(size_data)\n",
    "        epoch_kld += kld_loss.item() * len(size_data)\n",
    "        epoch_recon += recon_loss.item() * len(size_data)\n",
    "        epoch_max_loss = max(epoch_max_loss, max_recon_loss.item())\n",
    "        sample_num += len(size_data)\n",
    "\n",
    "    epoch_loss /= sample_num\n",
    "    epoch_recon /= sample_num\n",
    "    epoch_kld /= sample_num\n",
    "    return epoch_loss, epoch_recon, epoch_kld, epoch_max_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(decoder,sizedata,locality,latent_dim,locality_onehots_dict,step=0, test_size=100):\n",
    "    t0=time.time()\n",
    "    # bins=20\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        condition = random.sample(locality_onehots_dict.keys(),min(len(locality_onehots_dict.keys()),500))\n",
    "        condition = [eval(i) for i in condition]\n",
    "        condition=torch.Tensor(np.array(condition)).float().to(device)\n",
    "        c_min_99 = []\n",
    "        c_min_aver = []\n",
    "        r_min_99 = []\n",
    "        r_min_aver = []\n",
    "        coverage = []\n",
    "        for cond in condition:\n",
    "            z=torch.randn([test_size,latent_dim]).to(device)\n",
    "            y=decoder(z,cond.expand(test_size,-1)).cpu()\n",
    "        # print(y.shape)#y.shape=[test_size, 类别数=65]\n",
    "            locality_key = str(list(np.array(cond.cpu())))\n",
    "            matrix=torch.zeros((test_size,len(locality_onehots_dict[locality_key])))\n",
    "            for i in range(test_size):\n",
    "                for j in range(len(locality_onehots_dict[locality_key])): \n",
    "                    matrix[i,j]=torch.max(np.abs(y[i]-sizedata[locality_onehots_dict[locality_key][j]]))   \n",
    "            # for j in range(len(dataset)):\n",
    "            #     matrix[i][j] = sum(abs(y[i]-dataset[j][0]))/dataset[j][0].shape[0]\n",
    "            column_min,column_posi=torch.min(matrix,dim=0)\n",
    "            row_min,row_posi=torch.min(matrix,dim=1)\n",
    "            c_min_99.append(np.percentile(column_min,95))\n",
    "            c_min_aver.append(np.mean(np.array(column_min)))\n",
    "            r_min_99.append(np.percentile(row_min,95))\n",
    "            r_min_aver.append(np.mean(np.array(row_min)))\n",
    "            row_posi=torch.unique(row_posi)\n",
    "            coverage.append(len(row_posi)/len(locality_onehots_dict[locality_key]))\n",
    "        # plt.hist(column_min,bins=bins,density=True,cumulative=True,color='blue')\n",
    "        # plt.hist(row_min,bins=bins,density=True,cumulative=True,color='yellow')\n",
    "        # plt.savefig('result/{date}/'.format(date=date)+str(step)+\".png\")\n",
    "\n",
    "        # print(\"coverage for testsize \"+str(test_size)+\" is :\"+str(len(row_posi)/test_size))\n",
    "        print(\"eval in\"+str(time.time()-t0)+\" //coverage is %.4f on average and is %.4f for the worst\" %\n",
    "              (np.mean(coverage), np.percentile(coverage,1)) +\n",
    "              \" //per sample error is %.4f on average and is %.4f for the worst99 and is %.4f for the worst50\" %(np.mean(r_min_aver),np.percentile(r_min_99,99),np.percentile(r_min_99,50)) +\n",
    "              \" //per source data error is %.4f on average and is %.4f for the worst99 and is %.4f for the worst50\" %(np.mean(c_min_aver),np.percentile(c_min_99,99),np.percentile(c_min_99,50)))\n",
    "        # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_locality(pairdata, freqpairs,pairsize):\n",
    "    latent_size = 128\n",
    "    \n",
    "    src_count, dst_count = defaultdict(int), defaultdict(int)\n",
    "    src_ip, dst_ip = [], []\n",
    "\n",
    "    for pair in freqpairs:\n",
    "        src_ip.append(pairdata[pair]['srcip'][0])\n",
    "        dst_ip.append(pairdata[pair]['dstip'][0])\n",
    "        src_count[pairdata[pair]['srcip'][0]] += 1\n",
    "        dst_count[pairdata[pair]['dstip'][0]] += 1\n",
    "\n",
    "    max_src_count = np.max(list(src_count.values()))\n",
    "    max_dst_count = np.max(list(dst_count.values()))\n",
    "    print(\"maxsrccount\"+str(max_src_count)+\"maxdstcount\"+str(max_dst_count))\n",
    "    condition_size = max_src_count + max_dst_count + 2\n",
    "\n",
    "    locality_strings = []\n",
    "    locality_onehots = []\n",
    "    locality_onehots_dict=defaultdict(list)\n",
    "    i = 0\n",
    "    for pair in freqpairs:\n",
    "        locality_strings.append(\"{src},{dst}\".format(src=src_count[pairdata[pair]['srcip'][0]], dst=dst_count[pairdata[pair]['dstip'][0]]))\n",
    "        locality_onehot = np.zeros(condition_size)\n",
    "        locality_onehot[src_count[pairdata[pair]['srcip'][0]]] = 1\n",
    "        locality_onehot[max_src_count + dst_count[pairdata[pair]['dstip'][0]]] = 1\n",
    "        locality_onehots.append(locality_onehot)\n",
    "        # locality_onehot.dtype=int\n",
    "        locality_onehots_dict[str(list(locality_onehot))].append(i)\n",
    "        i+=1\n",
    "    values, counts = np.unique(locality_strings, return_counts=True)\n",
    "    pair_counts = dict(zip(values, counts))\n",
    "\n",
    "    return locality_strings, np.array(locality_onehots), pair_counts, condition_size, locality_onehots_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/Encore-CVAE/size_cvae.ipynb 单元格 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m traces \u001b[39m=\u001b[39m \u001b[39m100000\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m pairdata, freqpairs, n_size, n_interval,pairsize \u001b[39m=\u001b[39m get_fb_data(traces)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m sizedata \u001b[39m=\u001b[39m get_data(pairdata, freqpairs, \u001b[39m'\u001b[39;49m\u001b[39msize_index\u001b[39;49m\u001b[39m'\u001b[39;49m, n_size)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m intervaldata \u001b[39m=\u001b[39m get_data(pairdata, freqpairs, \u001b[39m'\u001b[39m\u001b[39minterval_index\u001b[39m\u001b[39m'\u001b[39m, n_interval)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mread data in \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39mm \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m ((time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0) \u001b[39m/\u001b[39m \u001b[39m60\u001b[39m, (time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0) \u001b[39m%\u001b[39m \u001b[39m60\u001b[39m))\n",
      "File \u001b[0;32m~/Encore-CVAE/utils/get_dataset.py:38\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(pairdata, freqpairs, index, n)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m pair \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(pairs):\n\u001b[1;32m     37\u001b[0m     feature \u001b[39m=\u001b[39m pairdata[freqpairs[pair]][index]\u001b[39m.\u001b[39miloc[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 38\u001b[0m     freq \u001b[39m=\u001b[39m feature\u001b[39m.\u001b[39;49mvalue_counts()\u001b[39m.\u001b[39msort_index()\n\u001b[1;32m     39\u001b[0m     \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m freq\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     40\u001b[0m         data[pair][key] \u001b[39m=\u001b[39m value\n",
      "File \u001b[0;32m~/anaconda3/envs/plf/lib/python3.10/site-packages/pandas/core/base.py:1010\u001b[0m, in \u001b[0;36mIndexOpsMixin.value_counts\u001b[0;34m(self, normalize, sort, ascending, bins, dropna)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[1;32m    924\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalue_counts\u001b[39m(\n\u001b[1;32m    925\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    930\u001b[0m     dropna: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    931\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series:\n\u001b[1;32m    932\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[39m    Return a Series containing counts of unique values.\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[39m    Name: count, dtype: int64\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1010\u001b[0m     \u001b[39mreturn\u001b[39;00m algorithms\u001b[39m.\u001b[39;49mvalue_counts_internal(\n\u001b[1;32m   1011\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1012\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m   1013\u001b[0m         ascending\u001b[39m=\u001b[39;49mascending,\n\u001b[1;32m   1014\u001b[0m         normalize\u001b[39m=\u001b[39;49mnormalize,\n\u001b[1;32m   1015\u001b[0m         bins\u001b[39m=\u001b[39;49mbins,\n\u001b[1;32m   1016\u001b[0m         dropna\u001b[39m=\u001b[39;49mdropna,\n\u001b[1;32m   1017\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/plf/lib/python3.10/site-packages/pandas/core/algorithms.py:938\u001b[0m, in \u001b[0;36mvalue_counts_internal\u001b[0;34m(values, sort, ascending, normalize, bins, dropna)\u001b[0m\n\u001b[1;32m    935\u001b[0m             idx \u001b[39m=\u001b[39m idx\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\n\u001b[1;32m    936\u001b[0m         idx\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m index_name\n\u001b[0;32m--> 938\u001b[0m         result \u001b[39m=\u001b[39m Series(counts, index\u001b[39m=\u001b[39;49midx, name\u001b[39m=\u001b[39;49mname, copy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    940\u001b[0m \u001b[39mif\u001b[39;00m sort:\n\u001b[1;32m    941\u001b[0m     result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39msort_values(ascending\u001b[39m=\u001b[39mascending)\n",
      "File \u001b[0;32m~/anaconda3/envs/plf/lib/python3.10/site-packages/pandas/core/series.py:516\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    514\u001b[0m manager \u001b[39m=\u001b[39m get_option(\u001b[39m\"\u001b[39m\u001b[39mmode.data_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mblock\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 516\u001b[0m     data \u001b[39m=\u001b[39m SingleBlockManager\u001b[39m.\u001b[39;49mfrom_array(data, index, refs\u001b[39m=\u001b[39;49mrefs)\n\u001b[1;32m    517\u001b[0m \u001b[39melif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    518\u001b[0m     data \u001b[39m=\u001b[39m SingleArrayManager\u001b[39m.\u001b[39mfrom_array(data, index)\n",
      "File \u001b[0;32m~/anaconda3/envs/plf/lib/python3.10/site-packages/pandas/core/internals/managers.py:1832\u001b[0m, in \u001b[0;36mSingleBlockManager.from_array\u001b[0;34m(cls, array, index, refs)\u001b[0m\n\u001b[1;32m   1825\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1826\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_array\u001b[39m(\n\u001b[1;32m   1827\u001b[0m     \u001b[39mcls\u001b[39m, array: ArrayLike, index: Index, refs: BlockValuesRefs \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1828\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SingleBlockManager:\n\u001b[1;32m   1829\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1830\u001b[0m \u001b[39m    Constructor for if we have an array that is not yet a Block.\u001b[39;00m\n\u001b[1;32m   1831\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1832\u001b[0m     array \u001b[39m=\u001b[39m maybe_coerce_values(array)\n\u001b[1;32m   1833\u001b[0m     bp \u001b[39m=\u001b[39m BlockPlacement(\u001b[39mslice\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(index)))\n\u001b[1;32m   1834\u001b[0m     block \u001b[39m=\u001b[39m new_block(array, placement\u001b[39m=\u001b[39mbp, ndim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, refs\u001b[39m=\u001b[39mrefs)\n",
      "File \u001b[0;32m~/anaconda3/envs/plf/lib/python3.10/site-packages/pandas/core/internals/blocks.py:2334\u001b[0m, in \u001b[0;36mmaybe_coerce_values\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   2331\u001b[0m \u001b[39m# Caller is responsible for ensuring NumpyExtensionArray is already extracted.\u001b[39;00m\n\u001b[1;32m   2333\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(values, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m-> 2334\u001b[0m     values \u001b[39m=\u001b[39m ensure_wrapped_if_datetimelike(values)\n\u001b[1;32m   2336\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   2337\u001b[0m         values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(values, dtype\u001b[39m=\u001b[39m\u001b[39mobject\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/plf/lib/python3.10/site-packages/pandas/core/construction.py:488\u001b[0m, in \u001b[0;36mensure_wrapped_if_datetimelike\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mensure_wrapped_if_datetimelike\u001b[39m(arr):\n\u001b[1;32m    485\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[39m    Wrap datetime64 and timedelta64 ndarrays in DatetimeArray/TimedeltaArray.\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(arr, np\u001b[39m.\u001b[39;49mndarray):\n\u001b[1;32m    489\u001b[0m         \u001b[39mif\u001b[39;00m arr\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    490\u001b[0m             \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m \u001b[39mimport\u001b[39;00m DatetimeArray\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "random.seed(114514)\n",
    "# read data\n",
    "traces = 100000\n",
    "pairdata, freqpairs, n_size, n_interval,pairsize = get_fb_data(traces)\n",
    "sizedata = get_data(pairdata, freqpairs, 'size_index', n_size)\n",
    "intervaldata = get_data(pairdata, freqpairs, 'interval_index', n_interval)\n",
    "print('read data in %dm %ds' % ((time.time() - t0) / 60, (time.time() - t0) % 60))\n",
    "\n",
    "# get locality\n",
    "locality_strings, locality_onehots, pair_counts, condition_size, locality_onehots_dict = get_locality(pairdata, freqpairs,pairsize)\n",
    "print('get locality in %dm %ds' % ((time.time() - t0) / 60, (time.time() - t0) % 60))\n",
    "dataset = [pair for pair in zip(sizedata, locality_onehots)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dims = [768, 512, 256]\n",
    "latent_dim = 32\n",
    "encoder = SizeEncoder(n_size, condition_size, hidden_dims, latent_dim).to(device)\n",
    "hidden_dims.reverse()\n",
    "decoder = SizeDecoder(latent_dim, condition_size, hidden_dims, n_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = torch.load('model/2023-11-23-7/encoder2.pth')\n",
    "decoder = torch.load('model/2023-11-23-7/decoder2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start in: 2023/11/22 07:40:44\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "kld_weight = 1e-4#1e-4不行\n",
    "optimizer = torch.optim.Adam([{'params': encoder.parameters()}, {'params': decoder.parameters()}], lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.9)\n",
    "print('start in:', time.strftime(\"%Y/%m/%d %H:%M:%S\", time.localtime()))\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.now()\n",
    "date = '%s-%s-%s-%s' % (date.year, date.month, date.day, date.hour)\n",
    "if os.path.exists('model/{date}/'.format(date=date)):\n",
    "    os.system('rm -r model/{date}/'.format(date=date))\n",
    "os.makedirs('model/{date}/'.format(date=date))\n",
    "if os.path.exists('result/{date}/'.format(date=date)):\n",
    "    os.system('rm -r result/{date}/'.format(date=date))\n",
    "os.makedirs('result/{date}/'.format(date=date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kld_weight(epoch=0):\n",
    "    kld_max=1e-4\n",
    "    kld_min=1e-6\n",
    "    if epoch<1000:\n",
    "        return (kld_max-kld_min)*((epoch)/1000.0)+kld_min\n",
    "    else:\n",
    "        return kld_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=100, avg_loss=nan, kld=nan, recon=nan(max=0.00e+00), time=510.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3102201/4260640875.py:6: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  condition = random.sample(locality_onehots_dict.keys(),min(len(locality_onehots_dict.keys()),500))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval in46.665637493133545 //coverage is 0.2950 on average and is 0.0127 for the worst //per sample error is nan on average and is nan for the worst99 and is nan for the worst50 //per source data error is nan on average and is nan for the worst99 and is nan for the worst50\n",
      "epoch=200, avg_loss=nan, kld=nan, recon=nan(max=0.00e+00), time=829.58\n",
      "eval in50.23838663101196 //coverage is 0.2978 on average and is 0.0123 for the worst //per sample error is nan on average and is nan for the worst99 and is nan for the worst50 //per source data error is nan on average and is nan for the worst99 and is nan for the worst50\n",
      "epoch=300, avg_loss=nan, kld=nan, recon=nan(max=0.00e+00), time=1264.80\n",
      "eval in85.78566002845764 //coverage is 0.3279 on average and is 0.0128 for the worst //per sample error is nan on average and is nan for the worst99 and is nan for the worst50 //per source data error is nan on average and is nan for the worst99 and is nan for the worst50\n",
      "epoch=400, avg_loss=nan, kld=nan, recon=nan(max=0.00e+00), time=1819.21\n",
      "eval in84.83345937728882 //coverage is 0.3035 on average and is 0.0116 for the worst //per sample error is nan on average and is nan for the worst99 and is nan for the worst50 //per source data error is nan on average and is nan for the worst99 and is nan for the worst50\n",
      "epoch=500, avg_loss=nan, kld=nan, recon=nan(max=0.00e+00), time=2333.56\n",
      "eval in82.19171929359436 //coverage is 0.3084 on average and is 0.0139 for the worst //per sample error is nan on average and is nan for the worst99 and is nan for the worst50 //per source data error is nan on average and is nan for the worst99 and is nan for the worst50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/Encore-CVAE/size_cvae.ipynb 单元格 13\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100001\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     kld_weight \u001b[39m=\u001b[39m get_kld_weight(epoch)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     epoch_loss, epoch_recon, epoch_kld, max_loss \u001b[39m=\u001b[39m train(encoder, decoder, dataset, optimizer)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m epoch_loss\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39mand\u001b[39;00m epoch \u001b[39m%\u001b[39m print_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/root/Encore-CVAE/size_cvae.ipynb 单元格 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m kld_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(\u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m var \u001b[39m-\u001b[39m mu \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m var\u001b[39m.\u001b[39mexp(), dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m), dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m recon_loss \u001b[39m+\u001b[39m max_loss_weight \u001b[39m*\u001b[39m max_recon_loss \u001b[39m+\u001b[39m kld_weight \u001b[39m*\u001b[39m kld_loss\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(size_data)\n",
      "File \u001b[0;32m~/anaconda3/envs/plf/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/plf/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "avg_loss = 0\n",
    "min_loss = 1e9\n",
    "print_every = 100\n",
    "for epoch in range(100001):\n",
    "    kld_weight = get_kld_weight(epoch)\n",
    "    epoch_loss, epoch_recon, epoch_kld, max_loss = train(encoder, decoder, dataset, optimizer)\n",
    "    avg_loss += epoch_loss\n",
    "    if epoch and epoch % print_every == 0:\n",
    "        avg_loss /= print_every\n",
    "        cur_time = time.time()\n",
    "        print(\"epoch=%d, avg_loss=%.2e, kld=%.2f, recon=%.2e(max=%.2e), time=%.2f\" % (epoch, avg_loss, epoch_kld, epoch_recon, max_loss, cur_time - start_time))\n",
    "        if epoch % (print_every*10) == 0:\n",
    "            evaluate(decoder,sizedata,locality_onehots,latent_dim,locality_onehots_dict,epoch,1000)\n",
    "        else:\n",
    "            evaluate(decoder,sizedata,locality_onehots,latent_dim,locality_onehots_dict,epoch,100)\n",
    "        if avg_loss < min_loss:\n",
    "            min_loss = avg_loss\n",
    "            torch.save(encoder, 'model/{date}/encoder.pth'.format(date=date))\n",
    "            torch.save(decoder, 'model/{date}/decoder.pth'.format(date=date))\n",
    "            print('save model')\n",
    "        sys.stdout.flush()\n",
    "        if avg_loss < 1e-3:\n",
    "            break\n",
    "        avg_loss = 0\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
