{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time         \n",
    "from sklearn import metrics\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchsummary import summary\n",
    "from utils.get_dataset import *\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "import datetime\n",
    "import random  \n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, hidden_dims, latent_dim):\n",
    "        super(SizeEncoder, self).__init__()\n",
    "        self.encoder = nn.ModuleList()\n",
    "        in_dim = input_dim + condition_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.encoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim),\n",
    "                    nn.ReLU())\n",
    "                    # ,nn.LayerNorm(h_dim))\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor, c: Tensor) -> List[Tensor]:\n",
    "        x = torch.cat((x, c), dim=1)\n",
    "        for module in self.encoder:\n",
    "            x = module(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "        return [mu, log_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SizeDecoder(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, condition_dim, hidden_dims, output_dim):\n",
    "        super(SizeDecoder, self).__init__()\n",
    "        self.decoder = torch.nn.ModuleList()\n",
    "        in_dim = latent_dim + condition_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.decoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim,),\n",
    "                    nn.ReLU())\n",
    "                    # nn.LayerNorm(h_dim))\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.output = nn.Linear(hidden_dims[-1], output_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor, c: Tensor) -> List[Tensor]:\n",
    "        x = torch.cat((x, c), dim=1)\n",
    "        for module in self.decoder:\n",
    "            x = module(x)\n",
    "        result = self.output(x)\n",
    "        result = F.softmax(result, dim=1)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps * std + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, dataset, optimizer):\n",
    "    dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    epoch_loss, epoch_kld, epoch_recon, sample_num, epoch_max_loss = 0, 0, 0, 0, 0\n",
    "    max_loss_weight = 0.1\n",
    "    for size_data, condition in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        size_data, condition = size_data.float().to(device), condition.float().to(device)\n",
    "        mu, var = encoder(size_data, condition)\n",
    "        z = reparameterize(mu, var)#z.shape=(batch_size,latent_dim)\n",
    "        y = decoder(z, condition)\n",
    "        recon_loss = F.l1_loss(y, size_data)\n",
    "        max_recon_loss = torch.max(torch.abs(y - size_data).mean(dim=1))\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + var - mu ** 2 - var.exp(), dim = 1), dim = 0)\n",
    "        loss = recon_loss + max_loss_weight * max_recon_loss + kld_weight * kld_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * len(size_data)\n",
    "        epoch_kld += kld_loss.item() * len(size_data)\n",
    "        epoch_recon += recon_loss.item() * len(size_data)\n",
    "        epoch_max_loss = max(epoch_max_loss, max_recon_loss.item())\n",
    "        sample_num += len(size_data)\n",
    "\n",
    "    epoch_loss /= sample_num\n",
    "    epoch_recon /= sample_num\n",
    "    epoch_kld /= sample_num\n",
    "    return epoch_loss, epoch_recon, epoch_kld, epoch_max_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(decoder,sizedata,locality,latent_dim,locality_onehots_dict,step=0, test_size=100):\n",
    "    t0=time.time()\n",
    "    # bins=20\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        condition = random.sample(locality_onehots_dict.keys(),min(len(locality_onehots_dict.keys()),500))\n",
    "        condition = [eval(i) for i in condition]\n",
    "        condition=torch.Tensor(np.array(condition)).float().to(device)\n",
    "        c_min_95 = []\n",
    "        c_min_aver = []\n",
    "        r_min_95 = []\n",
    "        r_min_aver = []\n",
    "        coverage = []\n",
    "        for cond in condition:\n",
    "            z=torch.randn([test_size,latent_dim]).to(device)\n",
    "            y=decoder(z,cond.expand(test_size,-1)).cpu()\n",
    "        # print(y.shape)#y.shape=[test_size, 类别数=65]\n",
    "            locality_key = str(list(np.array(cond.cpu())))\n",
    "            matrix=torch.zeros((test_size,len(locality_onehots_dict[locality_key])))\n",
    "            for i in range(test_size):\n",
    "                for j in range(len(locality_onehots_dict[locality_key])): \n",
    "                    matrix[i,j]=torch.max(np.abs(y[i]-sizedata[locality_onehots_dict[locality_key][j]]))   \n",
    "            # for j in range(len(dataset)):\n",
    "            #     matrix[i][j] = sum(abs(y[i]-dataset[j][0]))/dataset[j][0].shape[0]\n",
    "            column_min,column_posi=torch.min(matrix,dim=0)\n",
    "            row_min,row_posi=torch.min(matrix,dim=1)\n",
    "            c_min_95.append(np.percentile(column_min,95))\n",
    "            c_min_aver.append(np.mean(np.array(column_min)))\n",
    "            r_min_95.append(np.percentile(row_min,95))\n",
    "            r_min_aver.append(np.mean(np.array(row_min)))\n",
    "            row_posi=torch.unique(row_posi)\n",
    "            coverage.append(len(row_posi)/len(locality_onehots_dict[locality_key]))\n",
    "        # plt.hist(column_min,bins=bins,density=True,cumulative=True,color='blue')\n",
    "        # plt.hist(row_min,bins=bins,density=True,cumulative=True,color='yellow')\n",
    "        # plt.savefig('result/{date}/'.format(date=date)+str(step)+\".png\")\n",
    "\n",
    "        # print(\"coverage for testsize \"+str(test_size)+\" is :\"+str(len(row_posi)/test_size))\n",
    "        print(\"eval in\"+str(time.time()-t0)+\" //coverage is %.2f on average and is %.2f for the worst\" %\n",
    "              (np.mean(coverage), np.percentile(coverage,1)) +\n",
    "              \" //per sample error is %.2f on average and is %.2f for the worst\" %(np.mean(r_min_aver),np.percentile(r_min_95,95)) +\n",
    "              \" //per source data error is %.2f on average and is %.2f for the worst\" %(np.mean(c_min_aver),np.percentile(c_min_95,95)))\n",
    "        # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_locality(pairdata, freqpairs,pairsize):\n",
    "    latent_size = 128\n",
    "    \n",
    "    src_count, dst_count = defaultdict(int), defaultdict(int)\n",
    "    src_ip, dst_ip = [], []\n",
    "\n",
    "    for pair in freqpairs:\n",
    "        src_ip.append(pairdata[pair]['srcip'][0])\n",
    "        dst_ip.append(pairdata[pair]['dstip'][0])\n",
    "        src_count[pairdata[pair]['srcip'][0]] += 1\n",
    "        dst_count[pairdata[pair]['dstip'][0]] += 1\n",
    "\n",
    "    max_src_count = np.max(list(src_count.values()))\n",
    "    max_dst_count = np.max(list(dst_count.values()))\n",
    "    print(\"maxsrccount\"+str(max_src_count)+\"maxdstcount\"+str(max_dst_count))\n",
    "    condition_size = max_src_count + max_dst_count + 2\n",
    "\n",
    "    locality_strings = []\n",
    "    locality_onehots = []\n",
    "    locality_onehots_dict=defaultdict(list)\n",
    "    i = 0\n",
    "    for pair in freqpairs:\n",
    "        locality_strings.append(\"{src},{dst}\".format(src=src_count[pairdata[pair]['srcip'][0]], dst=dst_count[pairdata[pair]['dstip'][0]]))\n",
    "        locality_onehot = np.zeros(condition_size)\n",
    "        locality_onehot[src_count[pairdata[pair]['srcip'][0]]] = 1\n",
    "        locality_onehot[max_src_count + dst_count[pairdata[pair]['dstip'][0]]] = 1\n",
    "        locality_onehots.append(locality_onehot)\n",
    "        # locality_onehot.dtype=int\n",
    "        locality_onehots_dict[str(list(locality_onehot))].append(i)\n",
    "        i+=1\n",
    "    values, counts = np.unique(locality_strings, return_counts=True)\n",
    "    pair_counts = dict(zip(values, counts))\n",
    "\n",
    "    return locality_strings, np.array(locality_onehots), pair_counts, condition_size, locality_onehots_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data in 4m 40s\n",
      "maxsrccount106maxdstcount129\n",
      "get locality in 4m 55s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "random.seed(114514)\n",
    "# read data\n",
    "traces = 100000\n",
    "pairdata, freqpairs, n_size, n_interval,pairsize = get_fb_data(traces)\n",
    "sizedata = get_data(pairdata, freqpairs, 'size_index', n_size)\n",
    "intervaldata = get_data(pairdata, freqpairs, 'interval_index', n_interval)\n",
    "print('read data in %dm %ds' % ((time.time() - t0) / 60, (time.time() - t0) % 60))\n",
    "\n",
    "# get locality\n",
    "locality_strings, locality_onehots, pair_counts, condition_size, locality_onehots_dict = get_locality(pairdata, freqpairs,pairsize)\n",
    "print('get locality in %dm %ds' % ((time.time() - t0) / 60, (time.time() - t0) % 60))\n",
    "dataset = [pair for pair in zip(sizedata, locality_onehots)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dims = [768, 512, 256]\n",
    "latent_dim = 32\n",
    "encoder = SizeEncoder(n_size, condition_size, hidden_dims, latent_dim).to(device)\n",
    "hidden_dims.reverse()\n",
    "decoder = SizeDecoder(latent_dim, condition_size, hidden_dims, n_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start in: 2023/11/21 06:50:20\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "kld_weight = 1e-4#1e-4不行\n",
    "optimizer = torch.optim.Adam([{'params': encoder.parameters()}, {'params': decoder.parameters()}], lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.9)\n",
    "print('start in:', time.strftime(\"%Y/%m/%d %H:%M:%S\", time.localtime()))\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.now()\n",
    "date = '%s-%s-%s-%s' % (date.year, date.month, date.day, date.hour)\n",
    "if os.path.exists('model/{date}/'.format(date=date)):\n",
    "    os.system('rm -r model/{date}/'.format(date=date))\n",
    "os.makedirs('model/{date}/'.format(date=date))\n",
    "if os.path.exists('result/{date}/'.format(date=date)):\n",
    "    os.system('rm -r result/{date}/'.format(date=date))\n",
    "os.makedirs('result/{date}/'.format(date=date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=100, avg_loss=3.89e-03, kld=3.02, recon=2.56e-03(max=1.62e-02), time=280.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_901753/2939991403.py:6: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  condition = random.sample(locality_onehots_dict.keys(),min(len(locality_onehots_dict.keys()),500))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval in50.57851529121399 //coverage is 0.83 on average and is 0.37 for the worst //per sample error is 0.04 on average and is 0.12 for the worst //per source data error is 0.03 on average and is 0.13 for the worst\n",
      "save model\n",
      "epoch=200, avg_loss=3.28e-03, kld=4.20, recon=2.29e-03(max=1.18e-02), time=612.25\n",
      "eval in48.573731422424316 //coverage is 0.89 on average and is 0.49 for the worst //per sample error is 0.04 on average and is 0.10 for the worst //per source data error is 0.02 on average and is 0.10 for the worst\n",
      "save model\n",
      "epoch=300, avg_loss=3.07e-03, kld=5.03, recon=2.12e-03(max=1.17e-02), time=945.02\n",
      "eval in46.287317991256714 //coverage is 0.92 on average and is 0.50 for the worst //per sample error is 0.04 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.10 for the worst\n",
      "save model\n",
      "epoch=400, avg_loss=2.98e-03, kld=5.35, recon=2.06e-03(max=1.17e-02), time=1289.19\n",
      "eval in51.90271496772766 //coverage is 0.91 on average and is 0.47 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.09 for the worst\n",
      "save model\n",
      "epoch=500, avg_loss=2.92e-03, kld=5.62, recon=2.00e-03(max=1.17e-02), time=1616.49\n",
      "eval in47.88387393951416 //coverage is 0.93 on average and is 0.50 for the worst //per sample error is 0.04 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.08 for the worst\n",
      "save model\n",
      "epoch=600, avg_loss=2.88e-03, kld=5.80, recon=1.96e-03(max=1.17e-02), time=1936.03\n",
      "eval in52.5959107875824 //coverage is 0.92 on average and is 0.56 for the worst //per sample error is 0.03 on average and is 0.10 for the worst //per source data error is 0.02 on average and is 0.07 for the worst\n",
      "save model\n",
      "epoch=700, avg_loss=2.85e-03, kld=5.89, recon=1.92e-03(max=1.17e-02), time=2280.92\n",
      "eval in56.85312080383301 //coverage is 0.93 on average and is 0.56 for the worst //per sample error is 0.03 on average and is 0.10 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=800, avg_loss=2.82e-03, kld=5.98, recon=1.90e-03(max=1.17e-02), time=2618.51\n",
      "eval in51.9192328453064 //coverage is 0.94 on average and is 0.55 for the worst //per sample error is 0.04 on average and is 0.10 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=900, avg_loss=2.80e-03, kld=6.07, recon=1.87e-03(max=1.17e-02), time=2971.55\n",
      "eval in45.71502614021301 //coverage is 0.94 on average and is 0.58 for the worst //per sample error is 0.04 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=1000, avg_loss=2.77e-03, kld=6.12, recon=1.85e-03(max=1.17e-02), time=3288.34\n",
      "eval in438.4798221588135 //coverage is 1.00 on average and is 0.92 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.01 on average and is 0.03 for the worst\n",
      "save model\n",
      "epoch=1100, avg_loss=2.75e-03, kld=6.16, recon=1.82e-03(max=1.17e-02), time=4140.13\n",
      "eval in91.05025243759155 //coverage is 0.93 on average and is 0.57 for the worst //per sample error is 0.04 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=1200, avg_loss=2.73e-03, kld=6.26, recon=1.80e-03(max=5.64e-03), time=4664.43\n",
      "eval in73.75663685798645 //coverage is 0.94 on average and is 0.60 for the worst //per sample error is 0.04 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=1300, avg_loss=2.70e-03, kld=6.30, recon=1.78e-03(max=5.77e-03), time=5204.64\n",
      "eval in87.80013847351074 //coverage is 0.93 on average and is 0.59 for the worst //per sample error is 0.03 on average and is 0.10 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=1400, avg_loss=2.68e-03, kld=6.35, recon=1.76e-03(max=5.63e-03), time=5719.07\n",
      "eval in59.116002559661865 //coverage is 0.93 on average and is 0.52 for the worst //per sample error is 0.03 on average and is 0.10 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=1500, avg_loss=2.67e-03, kld=6.41, recon=1.75e-03(max=5.54e-03), time=6051.60\n",
      "eval in43.9352343082428 //coverage is 0.94 on average and is 0.58 for the worst //per sample error is 0.04 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=1600, avg_loss=2.65e-03, kld=6.44, recon=1.73e-03(max=4.99e-03), time=6352.08\n",
      "eval in36.927170276641846 //coverage is 0.94 on average and is 0.58 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=1700, avg_loss=2.64e-03, kld=6.47, recon=1.72e-03(max=4.48e-03), time=6639.53\n",
      "eval in45.808175563812256 //coverage is 0.94 on average and is 0.60 for the worst //per sample error is 0.04 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=1800, avg_loss=2.63e-03, kld=6.52, recon=1.71e-03(max=5.19e-03), time=6963.33\n",
      "eval in45.21704864501953 //coverage is 0.94 on average and is 0.61 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=1900, avg_loss=2.62e-03, kld=6.57, recon=1.70e-03(max=6.81e-03), time=7283.90\n",
      "eval in44.444870948791504 //coverage is 0.94 on average and is 0.61 for the worst //per sample error is 0.03 on average and is 0.10 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=2000, avg_loss=2.61e-03, kld=6.58, recon=1.69e-03(max=3.66e-03), time=7614.94\n",
      "eval in357.5843915939331 //coverage is 1.00 on average and is 0.93 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.01 on average and is 0.03 for the worst\n",
      "save model\n",
      "epoch=2100, avg_loss=2.60e-03, kld=6.64, recon=1.68e-03(max=3.86e-03), time=8248.74\n",
      "eval in47.16662335395813 //coverage is 0.94 on average and is 0.56 for the worst //per sample error is 0.03 on average and is 0.10 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=2200, avg_loss=2.60e-03, kld=6.63, recon=1.67e-03(max=3.99e-03), time=8575.00\n",
      "eval in53.0467369556427 //coverage is 0.93 on average and is 0.55 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=2300, avg_loss=2.59e-03, kld=6.67, recon=1.66e-03(max=4.29e-03), time=8902.68\n",
      "eval in52.40359020233154 //coverage is 0.94 on average and is 0.56 for the worst //per sample error is 0.04 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=2400, avg_loss=2.58e-03, kld=6.70, recon=1.65e-03(max=6.27e-03), time=9209.25\n",
      "eval in40.72745323181152 //coverage is 0.93 on average and is 0.51 for the worst //per sample error is 0.04 on average and is 0.12 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=2500, avg_loss=2.58e-03, kld=6.69, recon=1.65e-03(max=3.76e-03), time=9480.72\n",
      "eval in34.45954775810242 //coverage is 0.94 on average and is 0.57 for the worst //per sample error is 0.04 on average and is 0.12 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=2600, avg_loss=2.57e-03, kld=6.72, recon=1.64e-03(max=3.89e-03), time=9749.56\n",
      "eval in37.19891095161438 //coverage is 0.93 on average and is 0.56 for the worst //per sample error is 0.03 on average and is 0.10 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=2700, avg_loss=2.57e-03, kld=6.74, recon=1.63e-03(max=7.46e-03), time=10049.56\n",
      "eval in45.30549883842468 //coverage is 0.95 on average and is 0.60 for the worst //per sample error is 0.04 on average and is 0.12 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=2800, avg_loss=2.56e-03, kld=6.75, recon=1.63e-03(max=3.38e-03), time=10407.15\n",
      "eval in42.200493812561035 //coverage is 0.95 on average and is 0.60 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=2900, avg_loss=2.55e-03, kld=6.76, recon=1.62e-03(max=5.11e-03), time=10684.47\n",
      "eval in35.55999422073364 //coverage is 0.94 on average and is 0.58 for the worst //per sample error is 0.03 on average and is 0.10 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=3000, avg_loss=2.55e-03, kld=6.78, recon=1.62e-03(max=3.45e-03), time=10954.05\n",
      "eval in324.321825504303 //coverage is 0.99 on average and is 0.88 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.01 on average and is 0.03 for the worst\n",
      "save model\n",
      "epoch=3100, avg_loss=2.55e-03, kld=6.77, recon=1.61e-03(max=4.33e-03), time=11539.32\n",
      "eval in45.423465728759766 //coverage is 0.94 on average and is 0.62 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=3200, avg_loss=2.54e-03, kld=6.81, recon=1.61e-03(max=5.08e-03), time=11857.69\n",
      "eval in45.40985727310181 //coverage is 0.94 on average and is 0.59 for the worst //per sample error is 0.04 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=3300, avg_loss=2.54e-03, kld=6.80, recon=1.60e-03(max=3.69e-03), time=12198.62\n",
      "eval in61.36333131790161 //coverage is 0.93 on average and is 0.58 for the worst //per sample error is 0.04 on average and is 0.12 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=3400, avg_loss=2.53e-03, kld=6.82, recon=1.60e-03(max=3.71e-03), time=12534.32\n",
      "eval in47.00821018218994 //coverage is 0.94 on average and is 0.60 for the worst //per sample error is 0.03 on average and is 0.12 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=3500, avg_loss=2.53e-03, kld=6.91, recon=1.59e-03(max=5.40e-03), time=12857.20\n",
      "eval in46.36878442764282 //coverage is 0.94 on average and is 0.58 for the worst //per sample error is 0.04 on average and is 0.12 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=3600, avg_loss=2.53e-03, kld=6.86, recon=1.59e-03(max=5.43e-03), time=13175.89\n",
      "eval in47.09468412399292 //coverage is 0.94 on average and is 0.56 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=3700, avg_loss=2.52e-03, kld=6.89, recon=1.59e-03(max=3.76e-03), time=13517.39\n",
      "eval in53.76959776878357 //coverage is 0.93 on average and is 0.61 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=3800, avg_loss=2.52e-03, kld=6.90, recon=1.58e-03(max=3.89e-03), time=13859.38\n",
      "eval in49.928407192230225 //coverage is 0.94 on average and is 0.55 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=3900, avg_loss=2.52e-03, kld=6.90, recon=1.58e-03(max=3.72e-03), time=14181.43\n",
      "eval in52.95800495147705 //coverage is 0.93 on average and is 0.60 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=4000, avg_loss=2.52e-03, kld=6.95, recon=1.58e-03(max=7.83e-03), time=14516.88\n",
      "eval in374.0606665611267 //coverage is 0.99 on average and is 0.90 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.01 on average and is 0.03 for the worst\n",
      "save model\n",
      "epoch=4100, avg_loss=2.51e-03, kld=6.88, recon=1.57e-03(max=4.78e-03), time=15171.54\n",
      "eval in47.91599130630493 //coverage is 0.94 on average and is 0.61 for the worst //per sample error is 0.04 on average and is 0.13 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=4200, avg_loss=2.51e-03, kld=6.94, recon=1.57e-03(max=6.77e-03), time=15493.29\n",
      "eval in49.6884241104126 //coverage is 0.93 on average and is 0.60 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=4300, avg_loss=2.51e-03, kld=6.92, recon=1.57e-03(max=6.06e-03), time=15798.45\n",
      "eval in50.18663573265076 //coverage is 0.94 on average and is 0.60 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=4400, avg_loss=2.51e-03, kld=6.91, recon=1.57e-03(max=4.06e-03), time=16135.37\n",
      "eval in48.6833119392395 //coverage is 0.94 on average and is 0.57 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=4500, avg_loss=2.50e-03, kld=6.97, recon=1.57e-03(max=6.03e-03), time=16472.16\n",
      "eval in48.573426246643066 //coverage is 0.94 on average and is 0.58 for the worst //per sample error is 0.04 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=4600, avg_loss=2.50e-03, kld=6.91, recon=1.57e-03(max=6.29e-03), time=16805.77\n",
      "eval in52.43031620979309 //coverage is 0.93 on average and is 0.59 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=4700, avg_loss=2.50e-03, kld=6.91, recon=1.56e-03(max=5.30e-03), time=17118.63\n",
      "eval in36.80471849441528 //coverage is 0.94 on average and is 0.57 for the worst //per sample error is 0.04 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=4800, avg_loss=2.50e-03, kld=6.95, recon=1.56e-03(max=7.54e-03), time=17433.76\n",
      "eval in52.31170988082886 //coverage is 0.93 on average and is 0.56 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=4900, avg_loss=2.50e-03, kld=6.96, recon=1.55e-03(max=3.83e-03), time=17778.80\n",
      "eval in47.98457932472229 //coverage is 0.94 on average and is 0.60 for the worst //per sample error is 0.04 on average and is 0.12 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=5000, avg_loss=2.49e-03, kld=6.98, recon=1.55e-03(max=6.76e-03), time=18139.99\n",
      "eval in684.7936177253723 //coverage is 1.00 on average and is 0.93 for the worst //per sample error is 0.03 on average and is 0.10 for the worst //per source data error is 0.01 on average and is 0.03 for the worst\n",
      "save model\n",
      "epoch=5100, avg_loss=2.49e-03, kld=6.99, recon=1.55e-03(max=4.58e-03), time=19493.59\n",
      "eval in104.63125562667847 //coverage is 0.95 on average and is 0.62 for the worst //per sample error is 0.04 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=5200, avg_loss=2.49e-03, kld=6.97, recon=1.55e-03(max=3.54e-03), time=20266.05\n",
      "eval in99.93809795379639 //coverage is 0.94 on average and is 0.61 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=5300, avg_loss=2.49e-03, kld=6.94, recon=1.55e-03(max=6.07e-03), time=20813.06\n",
      "eval in49.58066415786743 //coverage is 0.94 on average and is 0.62 for the worst //per sample error is 0.03 on average and is 0.12 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=5400, avg_loss=2.49e-03, kld=6.96, recon=1.54e-03(max=4.78e-03), time=21150.25\n",
      "eval in49.19431829452515 //coverage is 0.94 on average and is 0.60 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=5500, avg_loss=2.48e-03, kld=7.01, recon=1.54e-03(max=2.24e-02), time=21626.16\n",
      "eval in87.31816577911377 //coverage is 0.94 on average and is 0.61 for the worst //per sample error is 0.03 on average and is 0.12 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=5600, avg_loss=2.48e-03, kld=7.03, recon=1.54e-03(max=4.21e-03), time=22452.53\n",
      "eval in121.36795425415039 //coverage is 0.94 on average and is 0.54 for the worst //per sample error is 0.03 on average and is 0.12 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=5700, avg_loss=2.48e-03, kld=6.98, recon=1.54e-03(max=4.76e-03), time=23294.78\n",
      "eval in77.32568883895874 //coverage is 0.94 on average and is 0.57 for the worst //per sample error is 0.04 on average and is 0.13 for the worst //per source data error is 0.02 on average and is 0.05 for the worst\n",
      "save model\n",
      "epoch=5800, avg_loss=2.48e-03, kld=6.99, recon=1.54e-03(max=1.45e-02), time=24057.18\n",
      "eval in117.44779419898987 //coverage is 0.94 on average and is 0.60 for the worst //per sample error is 0.03 on average and is 0.10 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "epoch=5900, avg_loss=2.48e-03, kld=7.00, recon=1.54e-03(max=7.63e-03), time=24750.92\n",
      "eval in52.91982913017273 //coverage is 0.94 on average and is 0.58 for the worst //per sample error is 0.03 on average and is 0.11 for the worst //per source data error is 0.02 on average and is 0.06 for the worst\n",
      "save model\n",
      "epoch=6000, avg_loss=2.48e-03, kld=7.02, recon=1.54e-03(max=7.42e-03), time=25121.33\n",
      "eval in656.9793531894684 //coverage is 1.00 on average and is 0.94 for the worst //per sample error is 0.04 on average and is 0.12 for the worst //per source data error is 0.01 on average and is 0.03 for the worst\n",
      "save model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/Encore-CVAE/size_cvae.ipynb 单元格 12\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m print_every \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100001\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     epoch_loss, epoch_recon, epoch_kld, max_loss \u001b[39m=\u001b[39m train(encoder, decoder, dataset, optimizer)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m epoch_loss\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39mand\u001b[39;00m epoch \u001b[39m%\u001b[39m print_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/root/Encore-CVAE/size_cvae.ipynb 单元格 12\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m epoch_loss, epoch_kld, epoch_recon, sample_num, epoch_max_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m max_loss_weight \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m size_data, condition \u001b[39min\u001b[39;00m dataloader:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstarpub-docker/root/Encore-CVAE/size_cvae.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     size_data, condition \u001b[39m=\u001b[39m size_data\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device), condition\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/plf/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/plf/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/plf/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/plf/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "avg_loss = 0\n",
    "min_loss = 1e9\n",
    "print_every = 100\n",
    "for epoch in range(100001):\n",
    "    epoch_loss, epoch_recon, epoch_kld, max_loss = train(encoder, decoder, dataset, optimizer)\n",
    "    avg_loss += epoch_loss\n",
    "    if epoch and epoch % print_every == 0:\n",
    "        avg_loss /= print_every\n",
    "        cur_time = time.time()\n",
    "        print(\"epoch=%d, avg_loss=%.2e, kld=%.2f, recon=%.2e(max=%.2e), time=%.2f\" % (epoch, avg_loss, epoch_kld, epoch_recon, max_loss, cur_time - start_time))\n",
    "        if epoch % (print_every*10) == 0:\n",
    "            evaluate(decoder,sizedata,locality_onehots,latent_dim,locality_onehots_dict,epoch,1000)\n",
    "        else:\n",
    "            evaluate(decoder,sizedata,locality_onehots,latent_dim,locality_onehots_dict,epoch,100)\n",
    "        if avg_loss < min_loss:\n",
    "            min_loss = avg_loss\n",
    "            torch.save(encoder, 'model/{date}/encoder.pth'.format(date=date))\n",
    "            torch.save(decoder, 'model/{date}/decoder.pth'.format(date=date))\n",
    "            print('save model')\n",
    "        sys.stdout.flush()\n",
    "        if avg_loss < 1e-3:\n",
    "            break\n",
    "        avg_loss = 0\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
